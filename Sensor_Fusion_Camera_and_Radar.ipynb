{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omprabhu98/MEngCapstone2022/blob/main/Sensor_Fusion_Camera_and_Radar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBDj97y9k18"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vwz1UzMu9mF-"
      },
      "outputs": [],
      "source": [
        "# Tensorflow\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# I/O libraries\n",
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "\n",
        "# Helper libraries\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm\n",
        "import IPython\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Comment this out if you want to see Deprecation warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwkf2tnYQKAT",
        "outputId": "98d4998f-c98a-47cd-ceb9-15f0490b9339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.7)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (17.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install timm;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdsigyfAhITX",
        "outputId": "d4fa4e4e-7e54-4e10-9a3d-beb941c74877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MEngCapstone2022'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 310 (delta 50), reused 27 (delta 15), pack-reused 222\u001b[K\n",
            "Receiving objects: 100% (310/310), 217.92 MiB | 31.63 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n",
            "Downloading Point_Cloud/sensor_points_100_frames.npy (641 MB)\n",
            "Error downloading object: Point_Cloud/sensor_points_100_frames.npy (e6a9e86): Smudge error: Error downloading Point_Cloud/sensor_points_100_frames.npy (e6a9e864134afd2654ffba80034f0350edcb1fc173f3830c7ab1f446e2e4934c): batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
            "\n",
            "Errors logged to /content/MEngCapstone2022/.git/lfs/logs/20231015T213617.528630255.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: Point_Cloud/sensor_points_100_frames.npy: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "MEngCapstone2022  sample_data\n",
            "'Depth Prediction'\t\t\t     README.md\n",
            "'Image Segmentation'\t\t\t     SAR+Camera_Fusion\n",
            " Image_Segmentation_Depth_Prediction.ipynb   Sensor_Fusion_Camera_and_Radar.ipynb\n",
            " Point_Cloud\t\t\t\t     Videos\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/omprabhu98/MEngCapstone2022.git\n",
        "!ls\n",
        "os.chdir(\"MEngCapstone2022\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZANbjAERfC5",
        "outputId": "d3931e2b-a4d7-495b-efb2-80ef4b7cbf9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKEwPDK-Edp"
      },
      "source": [
        "# Functions for Image Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Xfj_GZ-FWL"
      },
      "outputs": [],
      "source": [
        "class DeepLabModel(object):\n",
        "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, tarball_path):\n",
        "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "\n",
        "        # Extract frozen graph from tar archive.\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
        "        \"\"\"Runs inference on a single image.\n",
        "\n",
        "        Args:\n",
        "            image: A PIL.Image object, raw input image.\n",
        "            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
        "            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
        "\n",
        "        Returns:\n",
        "            resized_image: RGB image resized from original input image.\n",
        "            seg_map: Segmentation map of `resized_image`.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        target_size = (2049,1025)  # size of Cityscapes images\n",
        "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "        batch_seg_map = self.sess.run(\n",
        "            OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
        "        if len(seg_map.shape) == 2:\n",
        "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
        "        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)\n",
        "        return seg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIUhvfV-MiI"
      },
      "outputs": [],
      "source": [
        "def create_label_colormap():\n",
        "    \"\"\"Creates a label colormap used in Cityscapes segmentation benchmark.\n",
        "\n",
        "    Returns:\n",
        "        A Colormap for visualizing segmentation results.\n",
        "    \"\"\"\n",
        "    colormap = np.array([\n",
        "        [128,  64, 128],\n",
        "        [244,  35, 232],\n",
        "        [ 70,  70,  70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170,  30],\n",
        "        [220, 220,   0],\n",
        "        [107, 142,  35],\n",
        "        [152, 251, 152],\n",
        "        [ 70, 130, 180],\n",
        "        [220,  20,  60],\n",
        "        [255,   0,   0],\n",
        "        [  0,   0, 142],\n",
        "        [  0,   0,  70],\n",
        "        [  0,  60, 100],\n",
        "        [  0,  80, 100],\n",
        "        [  0,   0, 230],\n",
        "        [119,  11,  32],\n",
        "        [  0,   0,   0]], dtype=np.uint8)\n",
        "    return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "    Args:\n",
        "        label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "    Returns:\n",
        "        result: A 2D array with floating type. The element of the array\n",
        "            is the color indexed by the corresponding element in the input label\n",
        "            to the PASCAL color map.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If label is not of rank 2 or its value is larger than color\n",
        "            map maximum entry.\n",
        "    \"\"\"\n",
        "    if label.ndim != 2:\n",
        "        raise ValueError('Expect 2-D input label')\n",
        "\n",
        "    colormap = create_label_colormap()\n",
        "\n",
        "    if np.max(label) >= len(colormap):\n",
        "        raise ValueError('label value too large.')\n",
        "\n",
        "    return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input image')\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "    plt.imshow(seg_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation map')\n",
        "\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(seg_image, alpha=0.7)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation overlay')\n",
        "\n",
        "    unique_labels = np.unique(seg_map)\n",
        "    ax = plt.subplot(grid_spec[3])\n",
        "    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "    ax.yaxis.tick_right()\n",
        "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "    plt.xticks([], [])\n",
        "    ax.tick_params(width=0.0)\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle', 'void'])\n",
        "\n",
        "COLOR_MAP = np.array([\n",
        "    [128,  64, 128],\n",
        "    [244,  35, 232],\n",
        "    [ 70,  70,  70],\n",
        "    [102, 102, 156],\n",
        "    [190, 153, 153],\n",
        "    [153, 153, 153],\n",
        "    [250, 170,  30],\n",
        "    [220, 220,   0],\n",
        "    [107, 142,  35],\n",
        "    [152, 251, 152],\n",
        "    [ 70, 130, 180],\n",
        "    [220,  20,  60],\n",
        "    [255,   0,   0],\n",
        "    [  0,   0, 142],\n",
        "    [  0,   0,  70],\n",
        "    [  0,  60, 100],\n",
        "    [  0,  80, 100],\n",
        "    [  0,   0, 230],\n",
        "    [119,  11,  32],\n",
        "    [  0,   0,   0]], dtype=np.uint8)\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKiAGG_-QYw",
        "outputId": "95155fec-fac7-4860-a9e7-a0995dac6cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading model, this might take a while...\n",
            "download completed! loading DeepLab model...\n",
            "model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
        "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_cityscapes_trainfine':\n",
        "        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
        "    'xception65_cityscapes_trainfine':\n",
        "        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnUbIVY96HU"
      },
      "source": [
        "# Initialize Midas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW1E_emiQatf",
        "outputId": "c1a41e24-fbeb-4d65-9cb3-68a4939504d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJyJpTzQtDR",
        "outputId": "69c3c785-a965-4c79-f229-af1decfe9fd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DPTDepthModel(\n",
              "  (pretrained): Module(\n",
              "    (model): VisionTransformer(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "        (norm): Identity()\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (patch_drop): Identity()\n",
              "      (norm_pre): Identity()\n",
              "      (blocks): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (3): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (4): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (5): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (6): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (7): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (8): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (9): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (10): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (11): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (12): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (13): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (14): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (15): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (16): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (17): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (18): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (19): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (20): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (21): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (22): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (23): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (fc_norm): Identity()\n",
              "      (head_drop): Dropout(p=0.0, inplace=False)\n",
              "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "    )\n",
              "    (act_postprocess1): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
              "    )\n",
              "    (act_postprocess2): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (act_postprocess3): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (act_postprocess4): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (scratch): Module(\n",
              "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (refinenet1): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet2): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet3): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet4): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Interpolate()\n",
              "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Identity()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Da96O0Q06Z",
        "outputId": "f0fe33ea-564c-4333-e008-507f1dce0c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0kU6NJ1Ye5",
        "outputId": "d8f89953-d593-4118-a038-602618fc6637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja3q7PyhB4H"
      },
      "source": [
        "# Conversion to meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoPzjnl5hCCS"
      },
      "outputs": [],
      "source": [
        "nb_photo=34\n",
        "\n",
        "equiv=[[0.001,51],    #for extrapolation\n",
        "      [0.1,45],     #premier plan\n",
        "      [0.9,42.3],\n",
        "      [1.8,37.4],\n",
        "      [2.7,28.7],\n",
        "      [3.6,24.443365],\n",
        "      [4.5,22.058018],\n",
        "      [5.4,15.317413],\n",
        "      [6.3,14.677493],\n",
        "      [7.2,10.969739],\n",
        "      [8.1,10.883035],\n",
        "      [9,9.883035],\n",
        "      [9.9,8.058806],\n",
        "      [10.8,7.5158963],\n",
        "      [11.7,7.098169],\n",
        "      [12.6,6.111024],\n",
        "      [13.5,5.6323136],\n",
        "      [14.4,5.2216917],\n",
        "      [15.3,5],\n",
        "      [16.2,4.9529667],\n",
        "      [17.1,4.8],\n",
        "      [18,4.7],\n",
        "      [18.9,4.6],\n",
        "      [19.8,4.5],\n",
        "      [20.7,4.4],\n",
        "      [21.6,4.3],\n",
        "      [22.5,4.2],\n",
        "      [23.4,4.1],\n",
        "      [24.3,4],\n",
        "      [25.2,3.9],\n",
        "      [26.1,3.8],\n",
        "      [27,3.7],\n",
        "      [27.9,3.6],\n",
        "      [28.8,3.5],\n",
        "      [29.7,3.2],\n",
        "      [30.6,3],\n",
        "      [60,0.0],\n",
        "\n",
        "      [120,-6],    #horizon\n",
        "\n",
        "      [40,1.98],\n",
        "      [50,1.33]\n",
        "\n",
        "       ]   #for extrapolation\n",
        "\n",
        "#=========================================================================================\n",
        "\n",
        "equiv2=[[1,41.05157], #1yard  0302\n",
        "        [1,42.18351],\n",
        "        [1,31.304607],\n",
        "        [1,25.090006],\n",
        "        [1,23.275448], #5yard 0306\n",
        "        [1,19.171278],\n",
        "        [1,17.472866],\n",
        "        [1,16.775742],\n",
        "        [1,15.820402],\n",
        "        [1,15.538459], #10yard  0311\n",
        "        [1,14.466544],\n",
        "        [1,12.707126],\n",
        "        [1,10.957558],\n",
        "\n",
        "\n",
        "        [1,6.023936],#'''inacurrate'''\n",
        "        [1,9.797453],  #15yard 0316\n",
        "        [1,7.2150397],\n",
        "        [1,6.3944836],\n",
        "        [1,8.514687],\n",
        "        [1,7.735209],\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt4BAs1zhPp3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import Rbf, InterpolatedUnivariateSpline\n",
        "equiv=np.asarray(equiv)\n",
        "X = equiv[:,1]    #midas output\n",
        "Y=equiv[:,0]     #meters\n",
        "new_length = 25\n",
        "new_x = np.linspace(X.min(), X.max(), new_length)\n",
        "conv=sp.interpolate.interp1d(X, Y, kind='linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Fd1wKnhUqf"
      },
      "source": [
        "#Increased contrast\n",
        "Just to get increased contrast, not values in meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDq3tgxhQV4"
      },
      "outputs": [],
      "source": [
        "coef_expand=[[-5,-1],\n",
        "        [1,10],\n",
        "        [5,25],\n",
        "        [10,35],\n",
        "        [15,42],\n",
        "        [35,43],\n",
        "        [45,50],\n",
        "        [51,51]\n",
        "        ]\n",
        "coef_expand=np.asarray(coef_expand)\n",
        "X = coef_expand[:,0]    #midas output\n",
        "Y=coef_expand[:,1]      #meters\n",
        "expand=sp.interpolate.interp1d(X, Y, kind='cubic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99un1SR_xll"
      },
      "source": [
        "# Segmented Point Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-QtEEGNGcuH"
      },
      "outputs": [],
      "source": [
        "from math import sin,cos,atan2\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "def depth2pcd_segm(depth,seg_map):\n",
        "    # print(depth)\n",
        "    # print(seg_map)\n",
        "    width = depth.shape[1]\n",
        "    height = depth.shape[0]\n",
        "    # print(width)\n",
        "    # print(height)\n",
        "    fx= 926.9796142578125\n",
        "    fy= 924.431884765625\n",
        "    cx= 790.234375\n",
        "    cy= 617.5499267578125\n",
        "    points = []\n",
        "    objects_needed = {'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle'}\n",
        "    objects_car = {'car'}\n",
        "    objects_wanted = {'car', 'trees','sidewalk'}\n",
        "\n",
        "\n",
        "    for v in range(0, width, 5):\n",
        "        # for u in range(0, height, 2):\n",
        "        for u in range(0, 1000, 5):\n",
        "            R = depth[u][v]\n",
        "            color = seg_map[u][v]\n",
        "            # print(R)\n",
        "            # print(color)\n",
        "            if R == 0:\n",
        "                continue\n",
        "\n",
        "            X_cam = (v - cx)\n",
        "            Y_cam = -(u - cy)\n",
        "\n",
        "            theta_x = atan2(X_cam,fx)\n",
        "            theta_y = atan2(Y_cam,fy)\n",
        "\n",
        "            X = R*cos(theta_y)*sin(theta_x)\n",
        "            Y = R*cos(theta_x)*sin(theta_y)\n",
        "            Z = R*cos(theta_x)*cos(theta_y)\n",
        "\n",
        "            if LABEL_NAMES[color] in objects_car:\n",
        "              # points.append([X, Y, Z])\n",
        "              points.append([X, Y, Z, color])\n",
        "            # points.append([X, Y, Z,color])\n",
        "\n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "refmmrWb_2eH"
      },
      "outputs": [],
      "source": [
        "def prediction_stream(image, seg_map, seg_data, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    for i in range(len(seg_map)):\n",
        "        for j in range(len(seg_map[i])):\n",
        "                seg_data[i][j] = LABEL_NAMES[seg_map[i][j]]\n",
        "\n",
        "\n",
        "\n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    output = (output > 0) * output\n",
        "    distanceGuess = 2\n",
        "    alpha = output[output.shape[0]-10, int(output.shape[1]*3/4)]*distanceGuess\n",
        "    # output=expand(50*output/np.max(output)) # for better visualization\n",
        "    # output=conv(50*output/np.max(output))\n",
        "    output = alpha/(output+.001)\n",
        "    # print(output) # to get values in meters\n",
        "\n",
        "    # plt.imshow(output, cmap='plasma')\n",
        "\n",
        "    pc_3d = depth2pcd_segm(output,seg_map)\n",
        "    # if len(pc_3d) == 0:\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "    # else:\n",
        "    #   x = pc_3d[:, 0]\n",
        "    #   y = pc_3d[:, 1]\n",
        "    #   z = pc_3d[:, 2]\n",
        "    #   color = pc_3d[:, 3]\n",
        "    #   color_plot = np.zeros((len(color),3))\n",
        "    #   for i in range(len(color)):\n",
        "    #     color_plot[i] = COLOR_MAP[int(color[i])]\n",
        "\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "      # pc3dd = np.array([[x[i], z[i]] for i in range(len(x))])\n",
        "      # km = KMeans(n_clusters = 4)\n",
        "      # clusters= km.fit_predict(pc3dd)\n",
        "      # centroids = km.cluster_centers_\n",
        "      # points = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # distances will be used to calculate outliers\n",
        "      # distances = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # getting points and distances\n",
        "      # for i, center_elem in enumerate(centroids):\n",
        "      #     # cdist is used to calculate the distance between center and other points\n",
        "      #     distances = np.append(distances, cdist([center_elem],pc3dd[clusters == i], 'euclidean'))\n",
        "      #     points = np.append(points, pc3dd[clusters == i], axis=0)\n",
        "      # percentile = 90\n",
        "      # # getting outliers whose distances are greater than some percentile\n",
        "      # outliers = points[np.where(distances > np.percentile(distances, percentile))]\n",
        "      # #plotting outliers\n",
        "      # pc3dlast = C = np.array(list(filter(lambda x: x not in outliers, pc3dd)))\n",
        "      # plt.scatter(pc3dlast[:,0],pc3dlast[:,1],s=0.1)\n",
        "    #   plt.scatter(x,z,c=color_plot/255,s=0.1)\n",
        "    # plt.xlabel('Z')\n",
        "    # plt.ylabel('X')\n",
        "    # plt.xlim(-30, 30)\n",
        "    # plt.ylim(0, 30)\n",
        "\n",
        "    # plt.savefig('saved_figure.jpg')\n",
        "    # im = cv.imread('saved_figure.jpg')\n",
        "    # frames.append(im)\n",
        "    # plt.close()\n",
        "    # plt.imshow(expand(output), cmap='plasma')\n",
        "    # A = np.stack([seg_data,output])\n",
        "    # A = np.stack([seg_map,output])\n",
        "\n",
        "    # # Show visualization in a streaming fashion.\n",
        "    # f = BytesIO()\n",
        "    # plt.savefig(f, format='jpeg')\n",
        "    # IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "    # f.close()\n",
        "    # plt.close()\n",
        "    return pc_3d\n",
        "def prediction_video(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    seg_map = MODEL.run(original_im)\n",
        "    seg_data = np.full((len(seg_map),len(seg_map[0])),'nullvoidnada')\n",
        "    filled_seg_data = prediction_stream(original_im, seg_map, seg_data, frame, index)\n",
        "    # print(filled_seg_data)\n",
        "    return filled_seg_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-Kk28ldELA",
        "outputId": "d803beb6-c96b-4dd2-fcb5-7855b70fd09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e88d1a3e1d0b>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  SAR_tracklog = np.array(SAR_tracklog)\n"
          ]
        }
      ],
      "source": [
        "# Get Map of Time frames of SAR to Camera, by finding nearest frames\n",
        "\n",
        "import pickle\n",
        "# os.chdir(\"Capstone-Updated/SAR+Camera_Fusion\")\n",
        "\n",
        "with open('camera_times.pickle', 'rb') as file:\n",
        "    camera_times = pickle.load(file)\n",
        "\n",
        "with open('sar_tracklog.pickle', 'rb') as file:\n",
        "    SAR_tracklog = pickle.load(file)\n",
        "\n",
        "SAR_tracklog = np.array(SAR_tracklog)\n",
        "SAR_times = SAR_tracklog[:,0]\n",
        "\n",
        "timestamp_map = np.zeros(len(SAR_times))\n",
        "\n",
        "j=0\n",
        "for i in range(len(SAR_times)):\n",
        "  while camera_times[j]<SAR_times[i]:\n",
        "    j+=1\n",
        "  timestamp_map[i] = j\n",
        "# print(timestamp_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7a_YXpaEno-"
      },
      "outputs": [],
      "source": [
        "camera_frames = []\n",
        "\n",
        "SAMPLE_VIDEO = 'camera.mp4'\n",
        "\n",
        "\n",
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "total_frames = 1000\n",
        "\n",
        "try:\n",
        "    for i in range(total_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        camera_frames.append(frame)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_frames = 100\n",
        "pc3darray = []\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "      correct_frame = camera_frames[int(timestamp_map[i])]\n",
        "      filled_seg_DATA = prediction_video(correct_frame, int(timestamp_map[i]))\n",
        "      pc3darray.append(filled_seg_DATA)\n",
        "      IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "del camera_frames"
      ],
      "metadata": {
        "id": "cDTweVt5SlFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5OVRWSXYUo4"
      },
      "source": [
        "# Convert Camera Frame to SAR frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe_Qnn-DYafi"
      },
      "outputs": [],
      "source": [
        "#Transform a point [X,Y,Z] from the camera frame to the car frame (SAR)\n",
        "def Cam_ref_2_Car_ref(Pos_obj_cam):\n",
        "    #camera extrinsic (quaternion, translation)\n",
        "    R=[[ 0.99994752,  0.00325207,  0.00971481],\n",
        "    [-0.0030831 ,  0.99984459, -0.01735761],\n",
        "    [-0.00976975,  0.01732675,  0.99980215]]\n",
        "\n",
        "    T=[-0.41649988293647766, 0.09146018326282501, 0.011436160653829575]\n",
        "\n",
        "    Pos_obj_car = R@Pos_obj_cam[:3] + T\n",
        "    Pos_obj_car = np.append(Pos_obj_car,[Pos_obj_cam[-1]])\n",
        "    return Pos_obj_car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tucOmzqbSK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79ab3e6-dfd3-4699-8731-6668b591353a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-20877346f290>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  pc3darray_SAR_frame = np.array(pc3darray_SAR_frame)\n"
          ]
        }
      ],
      "source": [
        "pc3darray_SAR_frame = []\n",
        "\n",
        "for i in range(len(pc3darray)):\n",
        "  pointcloud = []\n",
        "  for j in range(len(pc3darray[i])):\n",
        "    pointcloud.append(Cam_ref_2_Car_ref(pc3darray[i][j]))\n",
        "  pc3darray_SAR_frame.append(pointcloud)\n",
        "\n",
        "# print(Cam_ref_2_Car_ref(pc3darray[10]))\n",
        "\n",
        "pc3darray_SAR_frame = np.array(pc3darray_SAR_frame)\n",
        "del pc3darray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKc6MK8Hy_YJ"
      },
      "source": [
        "# SAR Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ32WSdJzDmb"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# SHRAVANs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "RADAR_VIDEO = 'radar_sar.mp4'\n",
        "CAMERA_VIDEO = 'camera.mp4'\n",
        "\n",
        "# # OMs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "# RADAR_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/radar_sar.mp4'\n",
        "# CAMERA_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/camera.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqhiiwnSHaB8"
      },
      "outputs": [],
      "source": [
        "def Convert_to_Meters(frame):\n",
        "  center_x = 625\n",
        "  center_y = 624\n",
        "  height, width = frame.shape\n",
        "\n",
        "  points = []\n",
        "\n",
        "  for v in range(0, width):\n",
        "        for u in range(0, height):\n",
        "          if frame[u][v]<200:\n",
        "            x = (v-center_x)*0.04\n",
        "            y = -(u-center_y)*0.04\n",
        "            points.append([x,y])\n",
        "  return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlWuU-x-zXSF"
      },
      "outputs": [],
      "source": [
        "def GetThreshold_Binary(frame):\n",
        "  gray_scale = 255 - cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
        "  threshold = 0.9*np.max(gray_scale)\n",
        "  _, thres = cv.threshold(gray_scale, threshold, 255,cv.THRESH_BINARY)\n",
        "  point_cloud = Convert_to_Meters(thres)\n",
        "  point_cloud = np.array(point_cloud)\n",
        "\n",
        "  # fig = plt.figure(figsize=(12,6))\n",
        "  # plt.scatter(point_cloud[:,0],point_cloud[:,1],s=0.1)\n",
        "  # plt.xlabel('Z')\n",
        "  # plt.ylabel('X')\n",
        "  # plt.xlim(-25, 25)\n",
        "  # plt.ylim(0, 25)\n",
        "\n",
        "  # plt.savefig('saved_figure.jpg')\n",
        "  # im = cv.imread('saved_figure.jpg')\n",
        "  # frames.append(im)\n",
        "  # plt.close()\n",
        "\n",
        "  return thres, point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o--rPYmC54_a"
      },
      "outputs": [],
      "source": [
        "def Plot_Camera_with_SAR(point_cloud_SAR, point_cloud_Camera):\n",
        "\n",
        "  fig = plt.figure(figsize=(12,6))\n",
        "  plt.scatter(point_cloud_SAR[:,0],point_cloud_SAR[:,1],s=0.1, color = 'black')\n",
        "\n",
        "  if len(point_cloud_Camera) > 0:\n",
        "    color = (point_cloud_Camera[:, 3])\n",
        "    color_plot = np.array([COLOR_MAP[int(c)] for c in color])\n",
        "    plt.scatter(point_cloud_Camera[:,0],point_cloud_Camera[:,2],s=0.1 , color = color_plot/255)\n",
        "  plt.xlabel('Z')\n",
        "  plt.ylabel('X')\n",
        "  plt.xlim(-25, 25)\n",
        "  plt.ylim(0, 25)\n",
        "\n",
        "  plt.savefig('saved_figure.jpg')\n",
        "  im = cv.imread('saved_figure.jpg')\n",
        "  frames.append(im)\n",
        "  plt.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPgKg5mNzpBh"
      },
      "outputs": [],
      "source": [
        "frames=[]\n",
        "video_radar = cv.VideoCapture(RADAR_VIDEO)\n",
        "\n",
        "radar_thresholded = np.zeros((num_frames,624,1250))\n",
        "\n",
        "SAR_points = []\n",
        "\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video_radar.read()\n",
        "        if not _: break\n",
        "        radar_thresholded[i], points_SAR = GetThreshold_Binary(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Adaptive(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Gaussian(frame)\n",
        "\n",
        "        points_camera = np.array(pc3darray_SAR_frame[i])\n",
        "        Plot_Camera_with_SAR(points_SAR, points_camera)\n",
        "        SAR_points.append(points_SAR)\n",
        "\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "\n",
        "height, width, layers = frames[0].shape\n",
        "size = (width,height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv.VideoWriter('SARandCamera_pointcloud.avi', fourcc, 30.0, size)\n",
        "\n",
        "for i in range(len(frames)):\n",
        "    out.write(frames[i])\n",
        "out.release()\n",
        "\n",
        "\n",
        "SAR_points = np.array(radar_thresholded)\n",
        "Camera_points = np.array(pc3darray_SAR_frame)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sensor_points.npy', 'wb') as f:\n",
        "    np.save(f, SAR_points, allow_pickle= True)\n",
        "    np.save(f, Camera_points, allow_pickle = True)"
      ],
      "metadata": {
        "id": "Zklbd74lW8ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp sensor_points.npy /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "tT6y18w_cis9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filtering"
      ],
      "metadata": {
        "id": "oBi1_9momOAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "os.chdir(\"drive/MyDrive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25QRuGIlmSU1",
        "outputId": "e7708065-5515-4657-e50e-8c7113f538d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sensor_points_100_frames.npy', 'rb') as f:\n",
        "    sar_points = np.load(f, allow_pickle= True)\n",
        "    camera_points = np.load(f, allow_pickle= True)\n",
        "\n"
      ],
      "metadata": {
        "id": "uNqsFZZbYRBL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occupancy_radar_points = sar_points.copy()\n",
        "x_min = 1000\n",
        "z_min = 1000\n",
        "x_max = -1000\n",
        "z_max = -1000\n",
        "for i in range(len(camera_points)):\n",
        "  occupancy_map = {}\n",
        "  for j in range(len(camera_points[0])):\n",
        "    x = camera_points[i][j][0]\n",
        "    z = camera_points[i][j][2]\n",
        "\n",
        "    x_index = int(round(x/.04)) + 625\n",
        "    z_index = int(round(z/.04)) + 624\n",
        "    x_min = min(x_index,x_min)\n",
        "    z_min = min(z_index,z_min)\n",
        "    x_max = max(x_index,x_max)\n",
        "    z_max = max(z_index,z_max)\n",
        "\n",
        "    # print(x_index,z_index)\n",
        "    occupancy_map[(x_index,z_index)] = 1\n",
        "\n",
        "  for k in range(len(sar_points[0])):\n",
        "    for l in range(len(sar_points[0][0])):\n",
        "      # print(k,l)\n",
        "      # print(sar_points[i][k][l])\n",
        "      if ((l,k) in occupancy_map):\n",
        "          occupancy_radar_points[i][k][l] = 1\n",
        "      else:\n",
        "          occupancy_radar_points[i][k][l] = 0\n",
        "\n",
        "print(x_min,x_max)\n",
        "print(z_min,z_max)"
      ],
      "metadata": {
        "id": "PbmpoMd4TtyS",
        "outputId": "6b40263b-727e-45b1-e5af-17d23c921e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-bdeede314f65>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0moccupancy_radar_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0moccupancy_radar_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 624 is out of bounds for axis 0 with size 624"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(occupancy_radar_points[44])\n",
        "print(np.count_nonzero(occupancy_radar_points[44]))"
      ],
      "metadata": {
        "id": "mxxcCaH3eHa4",
        "outputId": "499a4201-7eed-44a9-f332-9ab2e70451be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(occupancy_radar_points[50], interpolation='nearest', origin='upper')\n"
      ],
      "metadata": {
        "id": "6KOzUdkbb1Vx",
        "outputId": "86d26595-48a0-442e-9c2b-a5fe7df266dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a42b74fc9a0>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(sar_points[30], interpolation='nearest', origin='upper')\n",
        "print(sar_points[44][582][800])\n",
        "# ax.imshow(sar_points[32], interpolation='nearest', origin='upper')\n"
      ],
      "metadata": {
        "id": "kCIa8m10YMdk",
        "outputId": "16f71a5f-0bf5-4b46-ef0a-b05e2a640647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bjBDj97y9k18",
        "3jKEwPDK-Edp",
        "Kja3q7PyhB4H",
        "s2Fd1wKnhUqf",
        "V5OVRWSXYUo4",
        "HKc6MK8Hy_YJ"
      ],
      "name": "SAR_and_Camera_PointCloud.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}