{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omprabhu98/MEngCapstone2022/blob/main/Sensor_Fusion_Camera_and_Radar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBDj97y9k18"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vwz1UzMu9mF-"
      },
      "outputs": [],
      "source": [
        "# Tensorflow\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# I/O libraries\n",
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "\n",
        "# Helper libraries\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm\n",
        "import IPython\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Comment this out if you want to see Deprecation warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwkf2tnYQKAT",
        "outputId": "98d4998f-c98a-47cd-ceb9-15f0490b9339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.7)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (17.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install timm;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdsigyfAhITX",
        "outputId": "d4fa4e4e-7e54-4e10-9a3d-beb941c74877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MEngCapstone2022'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 310 (delta 50), reused 27 (delta 15), pack-reused 222\u001b[K\n",
            "Receiving objects: 100% (310/310), 217.92 MiB | 31.63 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n",
            "Downloading Point_Cloud/sensor_points_100_frames.npy (641 MB)\n",
            "Error downloading object: Point_Cloud/sensor_points_100_frames.npy (e6a9e86): Smudge error: Error downloading Point_Cloud/sensor_points_100_frames.npy (e6a9e864134afd2654ffba80034f0350edcb1fc173f3830c7ab1f446e2e4934c): batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
            "\n",
            "Errors logged to /content/MEngCapstone2022/.git/lfs/logs/20231015T213617.528630255.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: Point_Cloud/sensor_points_100_frames.npy: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "MEngCapstone2022  sample_data\n",
            "'Depth Prediction'\t\t\t     README.md\n",
            "'Image Segmentation'\t\t\t     SAR+Camera_Fusion\n",
            " Image_Segmentation_Depth_Prediction.ipynb   Sensor_Fusion_Camera_and_Radar.ipynb\n",
            " Point_Cloud\t\t\t\t     Videos\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/omprabhu98/MEngCapstone2022.git\n",
        "!ls\n",
        "os.chdir(\"MEngCapstone2022\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZANbjAERfC5",
        "outputId": "26e80221-6545-4cd8-bd42-196a5057541b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKEwPDK-Edp"
      },
      "source": [
        "# Functions for Image Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Xfj_GZ-FWL"
      },
      "outputs": [],
      "source": [
        "class DeepLabModel(object):\n",
        "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, tarball_path):\n",
        "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "\n",
        "        # Extract frozen graph from tar archive.\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
        "        \"\"\"Runs inference on a single image.\n",
        "\n",
        "        Args:\n",
        "            image: A PIL.Image object, raw input image.\n",
        "            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
        "            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
        "\n",
        "        Returns:\n",
        "            resized_image: RGB image resized from original input image.\n",
        "            seg_map: Segmentation map of `resized_image`.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        target_size = (2049,1025)  # size of Cityscapes images\n",
        "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "        batch_seg_map = self.sess.run(\n",
        "            OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
        "        if len(seg_map.shape) == 2:\n",
        "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
        "        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)\n",
        "        return seg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIUhvfV-MiI"
      },
      "outputs": [],
      "source": [
        "def create_label_colormap():\n",
        "    \"\"\"Creates a label colormap used in Cityscapes segmentation benchmark.\n",
        "\n",
        "    Returns:\n",
        "        A Colormap for visualizing segmentation results.\n",
        "    \"\"\"\n",
        "    colormap = np.array([\n",
        "        [128,  64, 128],\n",
        "        [244,  35, 232],\n",
        "        [ 70,  70,  70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170,  30],\n",
        "        [220, 220,   0],\n",
        "        [107, 142,  35],\n",
        "        [152, 251, 152],\n",
        "        [ 70, 130, 180],\n",
        "        [220,  20,  60],\n",
        "        [255,   0,   0],\n",
        "        [  0,   0, 142],\n",
        "        [  0,   0,  70],\n",
        "        [  0,  60, 100],\n",
        "        [  0,  80, 100],\n",
        "        [  0,   0, 230],\n",
        "        [119,  11,  32],\n",
        "        [  0,   0,   0]], dtype=np.uint8)\n",
        "    return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "    Args:\n",
        "        label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "    Returns:\n",
        "        result: A 2D array with floating type. The element of the array\n",
        "            is the color indexed by the corresponding element in the input label\n",
        "            to the PASCAL color map.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If label is not of rank 2 or its value is larger than color\n",
        "            map maximum entry.\n",
        "    \"\"\"\n",
        "    if label.ndim != 2:\n",
        "        raise ValueError('Expect 2-D input label')\n",
        "\n",
        "    colormap = create_label_colormap()\n",
        "\n",
        "    if np.max(label) >= len(colormap):\n",
        "        raise ValueError('label value too large.')\n",
        "\n",
        "    return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input image')\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "    plt.imshow(seg_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation map')\n",
        "\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(seg_image, alpha=0.7)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation overlay')\n",
        "\n",
        "    unique_labels = np.unique(seg_map)\n",
        "    ax = plt.subplot(grid_spec[3])\n",
        "    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "    ax.yaxis.tick_right()\n",
        "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "    plt.xticks([], [])\n",
        "    ax.tick_params(width=0.0)\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle', 'void'])\n",
        "\n",
        "COLOR_MAP = np.array([\n",
        "    [128,  64, 128],\n",
        "    [244,  35, 232],\n",
        "    [ 70,  70,  70],\n",
        "    [102, 102, 156],\n",
        "    [190, 153, 153],\n",
        "    [153, 153, 153],\n",
        "    [250, 170,  30],\n",
        "    [220, 220,   0],\n",
        "    [107, 142,  35],\n",
        "    [152, 251, 152],\n",
        "    [ 70, 130, 180],\n",
        "    [220,  20,  60],\n",
        "    [255,   0,   0],\n",
        "    [  0,   0, 142],\n",
        "    [  0,   0,  70],\n",
        "    [  0,  60, 100],\n",
        "    [  0,  80, 100],\n",
        "    [  0,   0, 230],\n",
        "    [119,  11,  32],\n",
        "    [  0,   0,   0]], dtype=np.uint8)\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKiAGG_-QYw",
        "outputId": "95155fec-fac7-4860-a9e7-a0995dac6cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading model, this might take a while...\n",
            "download completed! loading DeepLab model...\n",
            "model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
        "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_cityscapes_trainfine':\n",
        "        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
        "    'xception65_cityscapes_trainfine':\n",
        "        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnUbIVY96HU"
      },
      "source": [
        "# Initialize Midas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW1E_emiQatf",
        "outputId": "c1a41e24-fbeb-4d65-9cb3-68a4939504d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJyJpTzQtDR",
        "outputId": "69c3c785-a965-4c79-f229-af1decfe9fd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DPTDepthModel(\n",
              "  (pretrained): Module(\n",
              "    (model): VisionTransformer(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "        (norm): Identity()\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (patch_drop): Identity()\n",
              "      (norm_pre): Identity()\n",
              "      (blocks): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (3): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (4): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (5): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (6): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (7): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (8): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (9): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (10): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (11): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (12): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (13): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (14): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (15): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (16): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (17): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (18): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (19): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (20): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (21): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (22): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (23): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (fc_norm): Identity()\n",
              "      (head_drop): Dropout(p=0.0, inplace=False)\n",
              "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "    )\n",
              "    (act_postprocess1): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
              "    )\n",
              "    (act_postprocess2): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (act_postprocess3): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (act_postprocess4): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (scratch): Module(\n",
              "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (refinenet1): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet2): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet3): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet4): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Interpolate()\n",
              "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Identity()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Da96O0Q06Z",
        "outputId": "f0fe33ea-564c-4333-e008-507f1dce0c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0kU6NJ1Ye5",
        "outputId": "d8f89953-d593-4118-a038-602618fc6637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja3q7PyhB4H"
      },
      "source": [
        "# Conversion to meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoPzjnl5hCCS"
      },
      "outputs": [],
      "source": [
        "nb_photo=34\n",
        "\n",
        "equiv=[[0.001,51],    #for extrapolation\n",
        "      [0.1,45],     #premier plan\n",
        "      [0.9,42.3],\n",
        "      [1.8,37.4],\n",
        "      [2.7,28.7],\n",
        "      [3.6,24.443365],\n",
        "      [4.5,22.058018],\n",
        "      [5.4,15.317413],\n",
        "      [6.3,14.677493],\n",
        "      [7.2,10.969739],\n",
        "      [8.1,10.883035],\n",
        "      [9,9.883035],\n",
        "      [9.9,8.058806],\n",
        "      [10.8,7.5158963],\n",
        "      [11.7,7.098169],\n",
        "      [12.6,6.111024],\n",
        "      [13.5,5.6323136],\n",
        "      [14.4,5.2216917],\n",
        "      [15.3,5],\n",
        "      [16.2,4.9529667],\n",
        "      [17.1,4.8],\n",
        "      [18,4.7],\n",
        "      [18.9,4.6],\n",
        "      [19.8,4.5],\n",
        "      [20.7,4.4],\n",
        "      [21.6,4.3],\n",
        "      [22.5,4.2],\n",
        "      [23.4,4.1],\n",
        "      [24.3,4],\n",
        "      [25.2,3.9],\n",
        "      [26.1,3.8],\n",
        "      [27,3.7],\n",
        "      [27.9,3.6],\n",
        "      [28.8,3.5],\n",
        "      [29.7,3.2],\n",
        "      [30.6,3],\n",
        "      [60,0.0],\n",
        "\n",
        "      [120,-6],    #horizon\n",
        "\n",
        "      [40,1.98],\n",
        "      [50,1.33]\n",
        "\n",
        "       ]   #for extrapolation\n",
        "\n",
        "#=========================================================================================\n",
        "\n",
        "equiv2=[[1,41.05157], #1yard  0302\n",
        "        [1,42.18351],\n",
        "        [1,31.304607],\n",
        "        [1,25.090006],\n",
        "        [1,23.275448], #5yard 0306\n",
        "        [1,19.171278],\n",
        "        [1,17.472866],\n",
        "        [1,16.775742],\n",
        "        [1,15.820402],\n",
        "        [1,15.538459], #10yard  0311\n",
        "        [1,14.466544],\n",
        "        [1,12.707126],\n",
        "        [1,10.957558],\n",
        "\n",
        "\n",
        "        [1,6.023936],#'''inacurrate'''\n",
        "        [1,9.797453],  #15yard 0316\n",
        "        [1,7.2150397],\n",
        "        [1,6.3944836],\n",
        "        [1,8.514687],\n",
        "        [1,7.735209],\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt4BAs1zhPp3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import Rbf, InterpolatedUnivariateSpline\n",
        "equiv=np.asarray(equiv)\n",
        "X = equiv[:,1]    #midas output\n",
        "Y=equiv[:,0]     #meters\n",
        "new_length = 25\n",
        "new_x = np.linspace(X.min(), X.max(), new_length)\n",
        "conv=sp.interpolate.interp1d(X, Y, kind='linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Fd1wKnhUqf"
      },
      "source": [
        "#Increased contrast\n",
        "Just to get increased contrast, not values in meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDq3tgxhQV4"
      },
      "outputs": [],
      "source": [
        "coef_expand=[[-5,-1],\n",
        "        [1,10],\n",
        "        [5,25],\n",
        "        [10,35],\n",
        "        [15,42],\n",
        "        [35,43],\n",
        "        [45,50],\n",
        "        [51,51]\n",
        "        ]\n",
        "coef_expand=np.asarray(coef_expand)\n",
        "X = coef_expand[:,0]    #midas output\n",
        "Y=coef_expand[:,1]      #meters\n",
        "expand=sp.interpolate.interp1d(X, Y, kind='cubic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99un1SR_xll"
      },
      "source": [
        "# Segmented Point Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-QtEEGNGcuH"
      },
      "outputs": [],
      "source": [
        "from math import sin,cos,atan2\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "def depth2pcd_segm(depth,seg_map):\n",
        "    # print(depth)\n",
        "    # print(seg_map)\n",
        "    width = depth.shape[1]\n",
        "    height = depth.shape[0]\n",
        "    # print(width)\n",
        "    # print(height)\n",
        "    fx= 926.9796142578125\n",
        "    fy= 924.431884765625\n",
        "    cx= 790.234375\n",
        "    cy= 617.5499267578125\n",
        "    points = []\n",
        "    objects_needed = {'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle'}\n",
        "    objects_car = {'car'}\n",
        "    objects_wanted = {'car', 'trees','sidewalk'}\n",
        "\n",
        "\n",
        "    for v in range(0, width, 5):\n",
        "        # for u in range(0, height, 2):\n",
        "        for u in range(0, 1000, 5):\n",
        "            R = depth[u][v]\n",
        "            color = seg_map[u][v]\n",
        "            # print(R)\n",
        "            # print(color)\n",
        "            if R == 0:\n",
        "                continue\n",
        "\n",
        "            X_cam = (v - cx)\n",
        "            Y_cam = -(u - cy)\n",
        "\n",
        "            theta_x = atan2(X_cam,fx)\n",
        "            theta_y = atan2(Y_cam,fy)\n",
        "\n",
        "            X = R*cos(theta_y)*sin(theta_x)\n",
        "            Y = R*cos(theta_x)*sin(theta_y)\n",
        "            Z = R*cos(theta_x)*cos(theta_y)\n",
        "\n",
        "            if LABEL_NAMES[color] in objects_car:\n",
        "              # points.append([X, Y, Z])\n",
        "              points.append([X, Y, Z, color])\n",
        "            # points.append([X, Y, Z,color])\n",
        "\n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "refmmrWb_2eH"
      },
      "outputs": [],
      "source": [
        "def prediction_stream(image, seg_map, seg_data, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    for i in range(len(seg_map)):\n",
        "        for j in range(len(seg_map[i])):\n",
        "                seg_data[i][j] = LABEL_NAMES[seg_map[i][j]]\n",
        "\n",
        "\n",
        "\n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    output = (output > 0) * output\n",
        "    distanceGuess = 2\n",
        "    alpha = output[output.shape[0]-10, int(output.shape[1]*3/4)]*distanceGuess\n",
        "    # output=expand(50*output/np.max(output)) # for better visualization\n",
        "    # output=conv(50*output/np.max(output))\n",
        "    output = alpha/(output+.001)\n",
        "    # print(output) # to get values in meters\n",
        "\n",
        "    # plt.imshow(output, cmap='plasma')\n",
        "\n",
        "    pc_3d = depth2pcd_segm(output,seg_map)\n",
        "    # if len(pc_3d) == 0:\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "    # else:\n",
        "    #   x = pc_3d[:, 0]\n",
        "    #   y = pc_3d[:, 1]\n",
        "    #   z = pc_3d[:, 2]\n",
        "    #   color = pc_3d[:, 3]\n",
        "    #   color_plot = np.zeros((len(color),3))\n",
        "    #   for i in range(len(color)):\n",
        "    #     color_plot[i] = COLOR_MAP[int(color[i])]\n",
        "\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "      # pc3dd = np.array([[x[i], z[i]] for i in range(len(x))])\n",
        "      # km = KMeans(n_clusters = 4)\n",
        "      # clusters= km.fit_predict(pc3dd)\n",
        "      # centroids = km.cluster_centers_\n",
        "      # points = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # distances will be used to calculate outliers\n",
        "      # distances = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # getting points and distances\n",
        "      # for i, center_elem in enumerate(centroids):\n",
        "      #     # cdist is used to calculate the distance between center and other points\n",
        "      #     distances = np.append(distances, cdist([center_elem],pc3dd[clusters == i], 'euclidean'))\n",
        "      #     points = np.append(points, pc3dd[clusters == i], axis=0)\n",
        "      # percentile = 90\n",
        "      # # getting outliers whose distances are greater than some percentile\n",
        "      # outliers = points[np.where(distances > np.percentile(distances, percentile))]\n",
        "      # #plotting outliers\n",
        "      # pc3dlast = C = np.array(list(filter(lambda x: x not in outliers, pc3dd)))\n",
        "      # plt.scatter(pc3dlast[:,0],pc3dlast[:,1],s=0.1)\n",
        "    #   plt.scatter(x,z,c=color_plot/255,s=0.1)\n",
        "    # plt.xlabel('Z')\n",
        "    # plt.ylabel('X')\n",
        "    # plt.xlim(-30, 30)\n",
        "    # plt.ylim(0, 30)\n",
        "\n",
        "    # plt.savefig('saved_figure.jpg')\n",
        "    # im = cv.imread('saved_figure.jpg')\n",
        "    # frames.append(im)\n",
        "    # plt.close()\n",
        "    # plt.imshow(expand(output), cmap='plasma')\n",
        "    # A = np.stack([seg_data,output])\n",
        "    # A = np.stack([seg_map,output])\n",
        "\n",
        "    # # Show visualization in a streaming fashion.\n",
        "    # f = BytesIO()\n",
        "    # plt.savefig(f, format='jpeg')\n",
        "    # IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "    # f.close()\n",
        "    # plt.close()\n",
        "    return pc_3d\n",
        "def prediction_video(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    seg_map = MODEL.run(original_im)\n",
        "    seg_data = np.full((len(seg_map),len(seg_map[0])),'nullvoidnada')\n",
        "    filled_seg_data = prediction_stream(original_im, seg_map, seg_data, frame, index)\n",
        "    # print(filled_seg_data)\n",
        "    return filled_seg_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-Kk28ldELA",
        "outputId": "d803beb6-c96b-4dd2-fcb5-7855b70fd09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e88d1a3e1d0b>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  SAR_tracklog = np.array(SAR_tracklog)\n"
          ]
        }
      ],
      "source": [
        "# Get Map of Time frames of SAR to Camera, by finding nearest frames\n",
        "\n",
        "import pickle\n",
        "# os.chdir(\"Capstone-Updated/SAR+Camera_Fusion\")\n",
        "\n",
        "with open('camera_times.pickle', 'rb') as file:\n",
        "    camera_times = pickle.load(file)\n",
        "\n",
        "with open('sar_tracklog.pickle', 'rb') as file:\n",
        "    SAR_tracklog = pickle.load(file)\n",
        "\n",
        "SAR_tracklog = np.array(SAR_tracklog)\n",
        "SAR_times = SAR_tracklog[:,0]\n",
        "\n",
        "timestamp_map = np.zeros(len(SAR_times))\n",
        "\n",
        "j=0\n",
        "for i in range(len(SAR_times)):\n",
        "  while camera_times[j]<SAR_times[i]:\n",
        "    j+=1\n",
        "  timestamp_map[i] = j\n",
        "# print(timestamp_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7a_YXpaEno-"
      },
      "outputs": [],
      "source": [
        "camera_frames = []\n",
        "\n",
        "SAMPLE_VIDEO = 'camera.mp4'\n",
        "\n",
        "\n",
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "total_frames = 1000\n",
        "\n",
        "try:\n",
        "    for i in range(total_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        camera_frames.append(frame)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_frames = 100\n",
        "pc3darray = []\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "      correct_frame = camera_frames[int(timestamp_map[i])]\n",
        "      filled_seg_DATA = prediction_video(correct_frame, int(timestamp_map[i]))\n",
        "      pc3darray.append(filled_seg_DATA)\n",
        "      IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "del camera_frames"
      ],
      "metadata": {
        "id": "cDTweVt5SlFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5OVRWSXYUo4"
      },
      "source": [
        "# Convert Camera Frame to SAR frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe_Qnn-DYafi"
      },
      "outputs": [],
      "source": [
        "#Transform a point [X,Y,Z] from the camera frame to the car frame (SAR)\n",
        "def Cam_ref_2_Car_ref(Pos_obj_cam):\n",
        "    #camera extrinsic (quaternion, translation)\n",
        "    R=[[ 0.99994752,  0.00325207,  0.00971481],\n",
        "    [-0.0030831 ,  0.99984459, -0.01735761],\n",
        "    [-0.00976975,  0.01732675,  0.99980215]]\n",
        "\n",
        "    T=[-0.41649988293647766, 0.09146018326282501, 0.011436160653829575]\n",
        "\n",
        "    Pos_obj_car = R@Pos_obj_cam[:3] + T\n",
        "    Pos_obj_car = np.append(Pos_obj_car,[Pos_obj_cam[-1]])\n",
        "    return Pos_obj_car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tucOmzqbSK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79ab3e6-dfd3-4699-8731-6668b591353a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-20877346f290>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  pc3darray_SAR_frame = np.array(pc3darray_SAR_frame)\n"
          ]
        }
      ],
      "source": [
        "pc3darray_SAR_frame = []\n",
        "\n",
        "for i in range(len(pc3darray)):\n",
        "  pointcloud = []\n",
        "  for j in range(len(pc3darray[i])):\n",
        "    pointcloud.append(Cam_ref_2_Car_ref(pc3darray[i][j]))\n",
        "  pc3darray_SAR_frame.append(pointcloud)\n",
        "\n",
        "# print(Cam_ref_2_Car_ref(pc3darray[10]))\n",
        "\n",
        "pc3darray_SAR_frame = np.array(pc3darray_SAR_frame)\n",
        "del pc3darray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKc6MK8Hy_YJ"
      },
      "source": [
        "# SAR Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ32WSdJzDmb"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# SHRAVANs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "RADAR_VIDEO = 'radar_sar.mp4'\n",
        "CAMERA_VIDEO = 'camera.mp4'\n",
        "\n",
        "# # OMs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "# RADAR_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/radar_sar.mp4'\n",
        "# CAMERA_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/camera.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqhiiwnSHaB8"
      },
      "outputs": [],
      "source": [
        "def Convert_to_Meters(frame):\n",
        "  center_x = 625\n",
        "  center_y = 624\n",
        "  height, width = frame.shape\n",
        "\n",
        "  points = []\n",
        "\n",
        "  for v in range(0, width):\n",
        "        for u in range(0, height):\n",
        "          if frame[u][v]<200:\n",
        "            x = (v-center_x)*0.04\n",
        "            y = -(u-center_y)*0.04\n",
        "            points.append([x,y])\n",
        "  return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlWuU-x-zXSF"
      },
      "outputs": [],
      "source": [
        "def GetThreshold_Binary(frame):\n",
        "  gray_scale = 255 - cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
        "  threshold = 0.9*np.max(gray_scale)\n",
        "  _, thres = cv.threshold(gray_scale, threshold, 255,cv.THRESH_BINARY)\n",
        "  point_cloud = Convert_to_Meters(thres)\n",
        "  point_cloud = np.array(point_cloud)\n",
        "\n",
        "  # fig = plt.figure(figsize=(12,6))\n",
        "  # plt.scatter(point_cloud[:,0],point_cloud[:,1],s=0.1)\n",
        "  # plt.xlabel('Z')\n",
        "  # plt.ylabel('X')\n",
        "  # plt.xlim(-25, 25)\n",
        "  # plt.ylim(0, 25)\n",
        "\n",
        "  # plt.savefig('saved_figure.jpg')\n",
        "  # im = cv.imread('saved_figure.jpg')\n",
        "  # frames.append(im)\n",
        "  # plt.close()\n",
        "\n",
        "  return thres, point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o--rPYmC54_a"
      },
      "outputs": [],
      "source": [
        "def Plot_Camera_with_SAR(point_cloud_SAR, point_cloud_Camera):\n",
        "\n",
        "  fig = plt.figure(figsize=(12,6))\n",
        "  plt.scatter(point_cloud_SAR[:,0],point_cloud_SAR[:,1],s=0.1, color = 'black')\n",
        "\n",
        "  if len(point_cloud_Camera) > 0:\n",
        "    color = (point_cloud_Camera[:, 3])\n",
        "    color_plot = np.array([COLOR_MAP[int(c)] for c in color])\n",
        "    plt.scatter(point_cloud_Camera[:,0],point_cloud_Camera[:,2],s=0.1 , color = color_plot/255)\n",
        "  plt.xlabel('Z')\n",
        "  plt.ylabel('X')\n",
        "  plt.xlim(-25, 25)\n",
        "  plt.ylim(0, 25)\n",
        "\n",
        "  plt.savefig('saved_figure.jpg')\n",
        "  im = cv.imread('saved_figure.jpg')\n",
        "  frames.append(im)\n",
        "  plt.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPgKg5mNzpBh"
      },
      "outputs": [],
      "source": [
        "frames=[]\n",
        "video_radar = cv.VideoCapture(RADAR_VIDEO)\n",
        "\n",
        "radar_thresholded = np.zeros((num_frames,624,1250))\n",
        "\n",
        "SAR_points = []\n",
        "\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video_radar.read()\n",
        "        if not _: break\n",
        "        radar_thresholded[i], points_SAR = GetThreshold_Binary(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Adaptive(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Gaussian(frame)\n",
        "\n",
        "        points_camera = np.array(pc3darray_SAR_frame[i])\n",
        "        Plot_Camera_with_SAR(points_SAR, points_camera)\n",
        "        SAR_points.append(points_SAR)\n",
        "\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "\n",
        "height, width, layers = frames[0].shape\n",
        "size = (width,height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv.VideoWriter('SARandCamera_pointcloud.avi', fourcc, 30.0, size)\n",
        "\n",
        "for i in range(len(frames)):\n",
        "    out.write(frames[i])\n",
        "out.release()\n",
        "\n",
        "\n",
        "SAR_points = np.array(radar_thresholded)\n",
        "Camera_points = np.array(pc3darray_SAR_frame)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sensor_points.npy', 'wb') as f:\n",
        "    np.save(f, SAR_points, allow_pickle= True)\n",
        "    np.save(f, Camera_points, allow_pickle = True)"
      ],
      "metadata": {
        "id": "Zklbd74lW8ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp sensor_points.npy /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "tT6y18w_cis9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filtering"
      ],
      "metadata": {
        "id": "oBi1_9momOAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "os.chdir(\"drive/MyDrive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25QRuGIlmSU1",
        "outputId": "248f930c-2fdb-4a41-a712-c91480993a8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sensor_points_100_frames.npy', 'rb') as f:\n",
        "    sar_points = np.load(f, allow_pickle= True)\n",
        "    camera_points = np.load(f, allow_pickle= True)\n",
        "\n"
      ],
      "metadata": {
        "id": "uNqsFZZbYRBL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occupancy_radar_points = sar_points.copy()\n",
        "x_min = 1000\n",
        "z_min = 1000\n",
        "x_max = -1000\n",
        "z_max = -1000\n",
        "print(\"k\",len(sar_points[0]))\n",
        "print(len(sar_points[0][0]))\n",
        "for i in range(len(camera_points)):\n",
        "  occupancy_map = {}\n",
        "  for j in range(len(camera_points[0])):\n",
        "    z = camera_points[i][j][0]\n",
        "    x = camera_points[i][j][2]\n",
        "    # print(x,z)\n",
        "    x_index = -1*int(x/.04) + 624\n",
        "    z_index = int(z/.04) + 650\n",
        "    x_min = min(x_index,x_min)\n",
        "    z_min = min(z_index,z_min)\n",
        "    x_max = max(x_index,x_max)\n",
        "    z_max = max(z_index,z_max)\n",
        "\n",
        "    # print(x_index,z_index)\n",
        "    occupancy_map[(x_index,z_index)] = 1\n",
        "\n",
        "  for k in range(len(sar_points[0])):\n",
        "    for l in range(len(sar_points[0][0])):\n",
        "      # print(k,l)\n",
        "      # print(l,k)\n",
        "      if ((k,l) in occupancy_map):\n",
        "          occupancy_radar_points[i][k][l] = 1\n",
        "      else:\n",
        "          occupancy_radar_points[i][k][l] = 0\n",
        "print(x_min,x_max)\n",
        "print(z_min,z_max)\n"
      ],
      "metadata": {
        "id": "PbmpoMd4TtyS",
        "outputId": "bddd87b6-76bf-4a5e-d2bc-cb480b7c1801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k 624\n",
            "1250\n",
            "-242 502\n",
            "160 739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(occupancy_radar_points[34])\n",
        "print(occupancy_map)\n",
        "print(np.count_nonzero(occupancy_radar_points))"
      ],
      "metadata": {
        "id": "mxxcCaH3eHa4",
        "outputId": "2bc0fd4f-a785-467b-fb94-94246f3d4690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "{(530, 488): 1, (531, 488): 1, (442, 316): 1, (444, 319): 1, (445, 322): 1, (447, 324): 1, (448, 327): 1, (450, 331): 1, (534, 482): 1, (535, 484): 1, (536, 486): 1, (441, 310): 1, (444, 317): 1, (446, 320): 1, (447, 322): 1, (448, 324): 1, (449, 326): 1, (450, 328): 1, (452, 330): 1, (453, 333): 1, (529, 472): 1, (530, 474): 1, (536, 484): 1, (441, 308): 1, (445, 315): 1, (447, 318): 1, (448, 321): 1, (449, 323): 1, (450, 324): 1, (451, 326): 1, (452, 329): 1, (453, 330): 1, (454, 333): 1, (535, 481): 1, (536, 483): 1, (537, 485): 1, (441, 305): 1, (445, 312): 1, (447, 316): 1, (448, 319): 1, (450, 321): 1, (451, 323): 1, (452, 325): 1, (453, 327): 1, (454, 329): 1, (455, 330): 1, (456, 332): 1, (536, 481): 1, (537, 483): 1, (445, 308): 1, (447, 313): 1, (449, 316): 1, (450, 318): 1, (451, 320): 1, (452, 322): 1, (453, 325): 1, (454, 327): 1, (455, 328): 1, (456, 330): 1, (447, 309): 1, (449, 313): 1, (450, 315): 1, (451, 318): 1, (452, 320): 1, (454, 322): 1, (455, 324): 1, (456, 326): 1, (457, 328): 1, (457, 329): 1, (444, 301): 1, (448, 308): 1, (450, 312): 1, (452, 315): 1, (453, 317): 1, (454, 320): 1, (455, 322): 1, (456, 324): 1, (457, 326): 1, (458, 328): 1, (459, 329): 1, (446, 301): 1, (449, 307): 1, (451, 311): 1, (453, 314): 1, (454, 316): 1, (455, 319): 1, (456, 321): 1, (457, 323): 1, (458, 325): 1, (459, 327): 1, (460, 329): 1, (461, 331): 1, (447, 300): 1, (450, 306): 1, (452, 310): 1, (454, 313): 1, (455, 315): 1, (456, 318): 1, (457, 320): 1, (459, 323): 1, (460, 324): 1, (461, 327): 1, (462, 329): 1, (462, 330): 1, (445, 292): 1, (449, 301): 1, (452, 305): 1, (453, 309): 1, (455, 312): 1, (456, 314): 1, (458, 317): 1, (459, 319): 1, (460, 322): 1, (461, 324): 1, (462, 326): 1, (463, 328): 1, (464, 329): 1, (447, 293): 1, (451, 300): 1, (453, 305): 1, (455, 308): 1, (456, 311): 1, (457, 314): 1, (459, 316): 1, (460, 319): 1, (461, 321): 1, (462, 323): 1, (463, 325): 1, (464, 327): 1, (465, 329): 1, (443, 281): 1, (446, 287): 1, (449, 294): 1, (452, 299): 1, (454, 303): 1, (456, 307): 1, (457, 310): 1, (459, 312): 1, (460, 315): 1, (461, 317): 1, (462, 320): 1, (463, 322): 1, (464, 324): 1, (465, 326): 1, (466, 328): 1, (441, 272): 1, (443, 278): 1, (446, 283): 1, (448, 288): 1, (451, 294): 1, (453, 298): 1, (455, 302): 1, (457, 305): 1, (458, 308): 1, (460, 311): 1, (461, 314): 1, (462, 316): 1, (463, 318): 1, (464, 320): 1, (465, 323): 1, (466, 325): 1, (467, 327): 1, (438, 263): 1, (442, 271): 1, (444, 275): 1, (446, 280): 1, (448, 284): 1, (451, 289): 1, (453, 294): 1, (455, 298): 1, (457, 302): 1, (458, 305): 1, (459, 308): 1, (461, 310): 1, (462, 313): 1, (463, 315): 1, (464, 317): 1, (465, 319): 1, (466, 322): 1, (467, 324): 1, (468, 326): 1, (436, 255): 1, (443, 268): 1, (446, 276): 1, (448, 279): 1, (449, 281): 1, (450, 284): 1, (453, 290): 1, (455, 294): 1, (457, 298): 1, (458, 301): 1, (460, 304): 1, (461, 307): 1, (462, 309): 1, (463, 312): 1, (464, 314): 1, (465, 316): 1, (466, 318): 1, (467, 321): 1, (469, 323): 1, (470, 326): 1, (433, 244): 1, (440, 258): 1, (446, 273): 1, (449, 278): 1, (450, 280): 1, (451, 282): 1, (452, 285): 1, (455, 290): 1, (456, 294): 1, (458, 297): 1, (460, 301): 1, (461, 303): 1, (462, 306): 1, (463, 308): 1, (464, 311): 1, (466, 313): 1, (467, 315): 1, (468, 317): 1, (469, 320): 1, (470, 323): 1, (471, 325): 1, (435, 244): 1, (444, 262): 1, (450, 275): 1, (451, 279): 1, (452, 281): 1, (453, 283): 1, (455, 287): 1, (457, 290): 1, (458, 294): 1, (460, 297): 1, (461, 300): 1, (462, 303): 1, (463, 305): 1, (464, 307): 1, (466, 310): 1, (467, 313): 1, (468, 315): 1, (469, 317): 1, (470, 319): 1, (471, 321): 1, (437, 244): 1, (447, 266): 1, (452, 275): 1, (453, 279): 1, (454, 281): 1, (455, 284): 1, (457, 287): 1, (458, 290): 1, (460, 293): 1, (461, 296): 1, (462, 299): 1, (463, 301): 1, (465, 304): 1, (466, 306): 1, (467, 309): 1, (468, 311): 1, (469, 314): 1, (470, 316): 1, (471, 318): 1, (472, 321): 1, (439, 243): 1, (451, 269): 1, (453, 275): 1, (455, 279): 1, (456, 281): 1, (457, 284): 1, (459, 287): 1, (460, 289): 1, (461, 292): 1, (462, 295): 1, (464, 298): 1, (465, 300): 1, (466, 303): 1, (467, 305): 1, (468, 308): 1, (469, 310): 1, (470, 312): 1, (471, 315): 1, (472, 317): 1, (473, 320): 1, (442, 245): 1, (452, 269): 1, (455, 275): 1, (457, 279): 1, (458, 281): 1, (459, 284): 1, (460, 286): 1, (461, 289): 1, (462, 291): 1}\n",
            "45073549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(occupancy_radar_points[50], interpolation='nearest', origin='upper')\n"
      ],
      "metadata": {
        "id": "6KOzUdkbb1Vx",
        "outputId": "70d20714-f46b-4f17-d1b1-1a82988ebc6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79ea0c9894b0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(sar_points[50], interpolation='nearest', origin='upper')\n",
        "print(sar_points[44][582][800])\n",
        "# ax.imshow(sar_points[32], interpolation='nearest', origin='upper')\n"
      ],
      "metadata": {
        "id": "kCIa8m10YMdk",
        "outputId": "52f1262f-e831-4495-edd8-6cd3ef19ebd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bjBDj97y9k18",
        "3jKEwPDK-Edp",
        "Kja3q7PyhB4H",
        "s2Fd1wKnhUqf",
        "V5OVRWSXYUo4",
        "HKc6MK8Hy_YJ"
      ],
      "name": "SAR_and_Camera_PointCloud.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}