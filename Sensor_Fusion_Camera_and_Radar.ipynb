{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omprabhu98/MEngCapstone2022/blob/main/Sensor_Fusion_Camera_and_Radar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBDj97y9k18"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vwz1UzMu9mF-"
      },
      "outputs": [],
      "source": [
        "# Tensorflow\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# I/O libraries\n",
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "\n",
        "# Helper libraries\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm\n",
        "import IPython\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Comment this out if you want to see Deprecation warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwkf2tnYQKAT",
        "outputId": "98d4998f-c98a-47cd-ceb9-15f0490b9339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.7)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (17.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install timm;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdsigyfAhITX",
        "outputId": "d4fa4e4e-7e54-4e10-9a3d-beb941c74877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MEngCapstone2022'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 310 (delta 50), reused 27 (delta 15), pack-reused 222\u001b[K\n",
            "Receiving objects: 100% (310/310), 217.92 MiB | 31.63 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n",
            "Downloading Point_Cloud/sensor_points_100_frames.npy (641 MB)\n",
            "Error downloading object: Point_Cloud/sensor_points_100_frames.npy (e6a9e86): Smudge error: Error downloading Point_Cloud/sensor_points_100_frames.npy (e6a9e864134afd2654ffba80034f0350edcb1fc173f3830c7ab1f446e2e4934c): batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.\n",
            "\n",
            "Errors logged to /content/MEngCapstone2022/.git/lfs/logs/20231015T213617.528630255.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: Point_Cloud/sensor_points_100_frames.npy: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "MEngCapstone2022  sample_data\n",
            "'Depth Prediction'\t\t\t     README.md\n",
            "'Image Segmentation'\t\t\t     SAR+Camera_Fusion\n",
            " Image_Segmentation_Depth_Prediction.ipynb   Sensor_Fusion_Camera_and_Radar.ipynb\n",
            " Point_Cloud\t\t\t\t     Videos\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/omprabhu98/MEngCapstone2022.git\n",
        "!ls\n",
        "os.chdir(\"MEngCapstone2022\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZANbjAERfC5",
        "outputId": "0a08a3eb-4a4f-4dc1-9cd5-a783f0385dcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKEwPDK-Edp"
      },
      "source": [
        "# Functions for Image Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Xfj_GZ-FWL"
      },
      "outputs": [],
      "source": [
        "class DeepLabModel(object):\n",
        "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, tarball_path):\n",
        "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "\n",
        "        # Extract frozen graph from tar archive.\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
        "        \"\"\"Runs inference on a single image.\n",
        "\n",
        "        Args:\n",
        "            image: A PIL.Image object, raw input image.\n",
        "            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
        "            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
        "\n",
        "        Returns:\n",
        "            resized_image: RGB image resized from original input image.\n",
        "            seg_map: Segmentation map of `resized_image`.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        target_size = (2049,1025)  # size of Cityscapes images\n",
        "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "        batch_seg_map = self.sess.run(\n",
        "            OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
        "        if len(seg_map.shape) == 2:\n",
        "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
        "        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)\n",
        "        return seg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIUhvfV-MiI"
      },
      "outputs": [],
      "source": [
        "def create_label_colormap():\n",
        "    \"\"\"Creates a label colormap used in Cityscapes segmentation benchmark.\n",
        "\n",
        "    Returns:\n",
        "        A Colormap for visualizing segmentation results.\n",
        "    \"\"\"\n",
        "    colormap = np.array([\n",
        "        [128,  64, 128],\n",
        "        [244,  35, 232],\n",
        "        [ 70,  70,  70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170,  30],\n",
        "        [220, 220,   0],\n",
        "        [107, 142,  35],\n",
        "        [152, 251, 152],\n",
        "        [ 70, 130, 180],\n",
        "        [220,  20,  60],\n",
        "        [255,   0,   0],\n",
        "        [  0,   0, 142],\n",
        "        [  0,   0,  70],\n",
        "        [  0,  60, 100],\n",
        "        [  0,  80, 100],\n",
        "        [  0,   0, 230],\n",
        "        [119,  11,  32],\n",
        "        [  0,   0,   0]], dtype=np.uint8)\n",
        "    return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "    Args:\n",
        "        label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "    Returns:\n",
        "        result: A 2D array with floating type. The element of the array\n",
        "            is the color indexed by the corresponding element in the input label\n",
        "            to the PASCAL color map.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If label is not of rank 2 or its value is larger than color\n",
        "            map maximum entry.\n",
        "    \"\"\"\n",
        "    if label.ndim != 2:\n",
        "        raise ValueError('Expect 2-D input label')\n",
        "\n",
        "    colormap = create_label_colormap()\n",
        "\n",
        "    if np.max(label) >= len(colormap):\n",
        "        raise ValueError('label value too large.')\n",
        "\n",
        "    return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input image')\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "    plt.imshow(seg_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation map')\n",
        "\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(seg_image, alpha=0.7)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation overlay')\n",
        "\n",
        "    unique_labels = np.unique(seg_map)\n",
        "    ax = plt.subplot(grid_spec[3])\n",
        "    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "    ax.yaxis.tick_right()\n",
        "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "    plt.xticks([], [])\n",
        "    ax.tick_params(width=0.0)\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle', 'void'])\n",
        "\n",
        "COLOR_MAP = np.array([\n",
        "    [128,  64, 128],\n",
        "    [244,  35, 232],\n",
        "    [ 70,  70,  70],\n",
        "    [102, 102, 156],\n",
        "    [190, 153, 153],\n",
        "    [153, 153, 153],\n",
        "    [250, 170,  30],\n",
        "    [220, 220,   0],\n",
        "    [107, 142,  35],\n",
        "    [152, 251, 152],\n",
        "    [ 70, 130, 180],\n",
        "    [220,  20,  60],\n",
        "    [255,   0,   0],\n",
        "    [  0,   0, 142],\n",
        "    [  0,   0,  70],\n",
        "    [  0,  60, 100],\n",
        "    [  0,  80, 100],\n",
        "    [  0,   0, 230],\n",
        "    [119,  11,  32],\n",
        "    [  0,   0,   0]], dtype=np.uint8)\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKiAGG_-QYw",
        "outputId": "95155fec-fac7-4860-a9e7-a0995dac6cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading model, this might take a while...\n",
            "download completed! loading DeepLab model...\n",
            "model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
        "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_cityscapes_trainfine':\n",
        "        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
        "    'xception65_cityscapes_trainfine':\n",
        "        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnUbIVY96HU"
      },
      "source": [
        "# Initialize Midas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW1E_emiQatf",
        "outputId": "c1a41e24-fbeb-4d65-9cb3-68a4939504d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJyJpTzQtDR",
        "outputId": "69c3c785-a965-4c79-f229-af1decfe9fd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DPTDepthModel(\n",
              "  (pretrained): Module(\n",
              "    (model): VisionTransformer(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "        (norm): Identity()\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (patch_drop): Identity()\n",
              "      (norm_pre): Identity()\n",
              "      (blocks): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (3): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (4): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (5): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (6): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (7): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (8): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (9): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (10): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (11): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (12): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (13): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (14): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (15): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (16): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (17): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (18): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (19): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (20): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (21): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (22): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (23): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (fc_norm): Identity()\n",
              "      (head_drop): Dropout(p=0.0, inplace=False)\n",
              "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "    )\n",
              "    (act_postprocess1): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
              "    )\n",
              "    (act_postprocess2): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (act_postprocess3): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (act_postprocess4): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (scratch): Module(\n",
              "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (refinenet1): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet2): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet3): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet4): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Interpolate()\n",
              "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Identity()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Da96O0Q06Z",
        "outputId": "f0fe33ea-564c-4333-e008-507f1dce0c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0kU6NJ1Ye5",
        "outputId": "d8f89953-d593-4118-a038-602618fc6637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja3q7PyhB4H"
      },
      "source": [
        "# Conversion to meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoPzjnl5hCCS"
      },
      "outputs": [],
      "source": [
        "nb_photo=34\n",
        "\n",
        "equiv=[[0.001,51],    #for extrapolation\n",
        "      [0.1,45],     #premier plan\n",
        "      [0.9,42.3],\n",
        "      [1.8,37.4],\n",
        "      [2.7,28.7],\n",
        "      [3.6,24.443365],\n",
        "      [4.5,22.058018],\n",
        "      [5.4,15.317413],\n",
        "      [6.3,14.677493],\n",
        "      [7.2,10.969739],\n",
        "      [8.1,10.883035],\n",
        "      [9,9.883035],\n",
        "      [9.9,8.058806],\n",
        "      [10.8,7.5158963],\n",
        "      [11.7,7.098169],\n",
        "      [12.6,6.111024],\n",
        "      [13.5,5.6323136],\n",
        "      [14.4,5.2216917],\n",
        "      [15.3,5],\n",
        "      [16.2,4.9529667],\n",
        "      [17.1,4.8],\n",
        "      [18,4.7],\n",
        "      [18.9,4.6],\n",
        "      [19.8,4.5],\n",
        "      [20.7,4.4],\n",
        "      [21.6,4.3],\n",
        "      [22.5,4.2],\n",
        "      [23.4,4.1],\n",
        "      [24.3,4],\n",
        "      [25.2,3.9],\n",
        "      [26.1,3.8],\n",
        "      [27,3.7],\n",
        "      [27.9,3.6],\n",
        "      [28.8,3.5],\n",
        "      [29.7,3.2],\n",
        "      [30.6,3],\n",
        "      [60,0.0],\n",
        "\n",
        "      [120,-6],    #horizon\n",
        "\n",
        "      [40,1.98],\n",
        "      [50,1.33]\n",
        "\n",
        "       ]   #for extrapolation\n",
        "\n",
        "#=========================================================================================\n",
        "\n",
        "equiv2=[[1,41.05157], #1yard  0302\n",
        "        [1,42.18351],\n",
        "        [1,31.304607],\n",
        "        [1,25.090006],\n",
        "        [1,23.275448], #5yard 0306\n",
        "        [1,19.171278],\n",
        "        [1,17.472866],\n",
        "        [1,16.775742],\n",
        "        [1,15.820402],\n",
        "        [1,15.538459], #10yard  0311\n",
        "        [1,14.466544],\n",
        "        [1,12.707126],\n",
        "        [1,10.957558],\n",
        "\n",
        "\n",
        "        [1,6.023936],#'''inacurrate'''\n",
        "        [1,9.797453],  #15yard 0316\n",
        "        [1,7.2150397],\n",
        "        [1,6.3944836],\n",
        "        [1,8.514687],\n",
        "        [1,7.735209],\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt4BAs1zhPp3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import Rbf, InterpolatedUnivariateSpline\n",
        "equiv=np.asarray(equiv)\n",
        "X = equiv[:,1]    #midas output\n",
        "Y=equiv[:,0]     #meters\n",
        "new_length = 25\n",
        "new_x = np.linspace(X.min(), X.max(), new_length)\n",
        "conv=sp.interpolate.interp1d(X, Y, kind='linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Fd1wKnhUqf"
      },
      "source": [
        "#Increased contrast\n",
        "Just to get increased contrast, not values in meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDq3tgxhQV4"
      },
      "outputs": [],
      "source": [
        "coef_expand=[[-5,-1],\n",
        "        [1,10],\n",
        "        [5,25],\n",
        "        [10,35],\n",
        "        [15,42],\n",
        "        [35,43],\n",
        "        [45,50],\n",
        "        [51,51]\n",
        "        ]\n",
        "coef_expand=np.asarray(coef_expand)\n",
        "X = coef_expand[:,0]    #midas output\n",
        "Y=coef_expand[:,1]      #meters\n",
        "expand=sp.interpolate.interp1d(X, Y, kind='cubic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99un1SR_xll"
      },
      "source": [
        "# Segmented Point Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-QtEEGNGcuH"
      },
      "outputs": [],
      "source": [
        "from math import sin,cos,atan2\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "def depth2pcd_segm(depth,seg_map):\n",
        "    # print(depth)\n",
        "    # print(seg_map)\n",
        "    width = depth.shape[1]\n",
        "    height = depth.shape[0]\n",
        "    # print(width)\n",
        "    # print(height)\n",
        "    fx= 926.9796142578125\n",
        "    fy= 924.431884765625\n",
        "    cx= 790.234375\n",
        "    cy= 617.5499267578125\n",
        "    points = []\n",
        "    objects_needed = {'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle'}\n",
        "    objects_car = {'car'}\n",
        "    objects_wanted = {'car', 'trees','sidewalk'}\n",
        "\n",
        "\n",
        "    for v in range(0, width, 5):\n",
        "        # for u in range(0, height, 2):\n",
        "        for u in range(0, 1000, 5):\n",
        "            R = depth[u][v]\n",
        "            color = seg_map[u][v]\n",
        "            # print(R)\n",
        "            # print(color)\n",
        "            if R == 0:\n",
        "                continue\n",
        "\n",
        "            X_cam = (v - cx)\n",
        "            Y_cam = -(u - cy)\n",
        "\n",
        "            theta_x = atan2(X_cam,fx)\n",
        "            theta_y = atan2(Y_cam,fy)\n",
        "\n",
        "            X = R*cos(theta_y)*sin(theta_x)\n",
        "            Y = R*cos(theta_x)*sin(theta_y)\n",
        "            Z = R*cos(theta_x)*cos(theta_y)\n",
        "\n",
        "            if LABEL_NAMES[color] in objects_car:\n",
        "              # points.append([X, Y, Z])\n",
        "              points.append([X, Y, Z, color])\n",
        "            # points.append([X, Y, Z,color])\n",
        "\n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "refmmrWb_2eH"
      },
      "outputs": [],
      "source": [
        "def prediction_stream(image, seg_map, seg_data, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    for i in range(len(seg_map)):\n",
        "        for j in range(len(seg_map[i])):\n",
        "                seg_data[i][j] = LABEL_NAMES[seg_map[i][j]]\n",
        "\n",
        "\n",
        "\n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    output = (output > 0) * output\n",
        "    distanceGuess = 2\n",
        "    alpha = output[output.shape[0]-10, int(output.shape[1]*3/4)]*distanceGuess\n",
        "    # output=expand(50*output/np.max(output)) # for better visualization\n",
        "    # output=conv(50*output/np.max(output))\n",
        "    output = alpha/(output+.001)\n",
        "    # print(output) # to get values in meters\n",
        "\n",
        "    # plt.imshow(output, cmap='plasma')\n",
        "\n",
        "    pc_3d = depth2pcd_segm(output,seg_map)\n",
        "    # if len(pc_3d) == 0:\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "    # else:\n",
        "    #   x = pc_3d[:, 0]\n",
        "    #   y = pc_3d[:, 1]\n",
        "    #   z = pc_3d[:, 2]\n",
        "    #   color = pc_3d[:, 3]\n",
        "    #   color_plot = np.zeros((len(color),3))\n",
        "    #   for i in range(len(color)):\n",
        "    #     color_plot[i] = COLOR_MAP[int(color[i])]\n",
        "\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "      # pc3dd = np.array([[x[i], z[i]] for i in range(len(x))])\n",
        "      # km = KMeans(n_clusters = 4)\n",
        "      # clusters= km.fit_predict(pc3dd)\n",
        "      # centroids = km.cluster_centers_\n",
        "      # points = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # distances will be used to calculate outliers\n",
        "      # distances = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # getting points and distances\n",
        "      # for i, center_elem in enumerate(centroids):\n",
        "      #     # cdist is used to calculate the distance between center and other points\n",
        "      #     distances = np.append(distances, cdist([center_elem],pc3dd[clusters == i], 'euclidean'))\n",
        "      #     points = np.append(points, pc3dd[clusters == i], axis=0)\n",
        "      # percentile = 90\n",
        "      # # getting outliers whose distances are greater than some percentile\n",
        "      # outliers = points[np.where(distances > np.percentile(distances, percentile))]\n",
        "      # #plotting outliers\n",
        "      # pc3dlast = C = np.array(list(filter(lambda x: x not in outliers, pc3dd)))\n",
        "      # plt.scatter(pc3dlast[:,0],pc3dlast[:,1],s=0.1)\n",
        "    #   plt.scatter(x,z,c=color_plot/255,s=0.1)\n",
        "    # plt.xlabel('Z')\n",
        "    # plt.ylabel('X')\n",
        "    # plt.xlim(-30, 30)\n",
        "    # plt.ylim(0, 30)\n",
        "\n",
        "    # plt.savefig('saved_figure.jpg')\n",
        "    # im = cv.imread('saved_figure.jpg')\n",
        "    # frames.append(im)\n",
        "    # plt.close()\n",
        "    # plt.imshow(expand(output), cmap='plasma')\n",
        "    # A = np.stack([seg_data,output])\n",
        "    # A = np.stack([seg_map,output])\n",
        "\n",
        "    # # Show visualization in a streaming fashion.\n",
        "    # f = BytesIO()\n",
        "    # plt.savefig(f, format='jpeg')\n",
        "    # IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "    # f.close()\n",
        "    # plt.close()\n",
        "    return pc_3d\n",
        "def prediction_video(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    seg_map = MODEL.run(original_im)\n",
        "    seg_data = np.full((len(seg_map),len(seg_map[0])),'nullvoidnada')\n",
        "    filled_seg_data = prediction_stream(original_im, seg_map, seg_data, frame, index)\n",
        "    # print(filled_seg_data)\n",
        "    return filled_seg_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-Kk28ldELA",
        "outputId": "d803beb6-c96b-4dd2-fcb5-7855b70fd09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e88d1a3e1d0b>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  SAR_tracklog = np.array(SAR_tracklog)\n"
          ]
        }
      ],
      "source": [
        "# Get Map of Time frames of SAR to Camera, by finding nearest frames\n",
        "\n",
        "import pickle\n",
        "# os.chdir(\"Capstone-Updated/SAR+Camera_Fusion\")\n",
        "\n",
        "with open('camera_times.pickle', 'rb') as file:\n",
        "    camera_times = pickle.load(file)\n",
        "\n",
        "with open('sar_tracklog.pickle', 'rb') as file:\n",
        "    SAR_tracklog = pickle.load(file)\n",
        "\n",
        "SAR_tracklog = np.array(SAR_tracklog)\n",
        "SAR_times = SAR_tracklog[:,0]\n",
        "\n",
        "timestamp_map = np.zeros(len(SAR_times))\n",
        "\n",
        "j=0\n",
        "for i in range(len(SAR_times)):\n",
        "  while camera_times[j]<SAR_times[i]:\n",
        "    j+=1\n",
        "  timestamp_map[i] = j\n",
        "# print(timestamp_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7a_YXpaEno-"
      },
      "outputs": [],
      "source": [
        "camera_frames = []\n",
        "\n",
        "SAMPLE_VIDEO = 'camera.mp4'\n",
        "\n",
        "\n",
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "total_frames = 1000\n",
        "\n",
        "try:\n",
        "    for i in range(total_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        camera_frames.append(frame)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_frames = 100\n",
        "pc3darray = []\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "      correct_frame = camera_frames[int(timestamp_map[i])]\n",
        "      filled_seg_DATA = prediction_video(correct_frame, int(timestamp_map[i]))\n",
        "      pc3darray.append(filled_seg_DATA)\n",
        "      IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "del camera_frames"
      ],
      "metadata": {
        "id": "cDTweVt5SlFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5OVRWSXYUo4"
      },
      "source": [
        "# Convert Camera Frame to SAR frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe_Qnn-DYafi"
      },
      "outputs": [],
      "source": [
        "#Transform a point [X,Y,Z] from the camera frame to the car frame (SAR)\n",
        "def Cam_ref_2_Car_ref(Pos_obj_cam):\n",
        "    #camera extrinsic (quaternion, translation)\n",
        "    R=[[ 0.99994752,  0.00325207,  0.00971481],\n",
        "    [-0.0030831 ,  0.99984459, -0.01735761],\n",
        "    [-0.00976975,  0.01732675,  0.99980215]]\n",
        "\n",
        "    T=[-0.41649988293647766, 0.09146018326282501, 0.011436160653829575]\n",
        "\n",
        "    Pos_obj_car = R@Pos_obj_cam[:3] + T\n",
        "    Pos_obj_car = np.append(Pos_obj_car,[Pos_obj_cam[-1]])\n",
        "    return Pos_obj_car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tucOmzqbSK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79ab3e6-dfd3-4699-8731-6668b591353a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-20877346f290>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  pc3darray_SAR_frame = np.array(pc3darray_SAR_frame)\n"
          ]
        }
      ],
      "source": [
        "pc3darray_SAR_frame = []\n",
        "\n",
        "for i in range(len(pc3darray)):\n",
        "  pointcloud = []\n",
        "  for j in range(len(pc3darray[i])):\n",
        "    pointcloud.append(Cam_ref_2_Car_ref(pc3darray[i][j]))\n",
        "  pc3darray_SAR_frame.append(pointcloud)\n",
        "\n",
        "# print(Cam_ref_2_Car_ref(pc3darray[10]))\n",
        "\n",
        "pc3darray_SAR_frame = np.array(pc3darray_SAR_frame)\n",
        "del pc3darray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKc6MK8Hy_YJ"
      },
      "source": [
        "# SAR Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ32WSdJzDmb"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# SHRAVANs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "RADAR_VIDEO = 'radar_sar.mp4'\n",
        "CAMERA_VIDEO = 'camera.mp4'\n",
        "\n",
        "# # OMs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "# RADAR_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/radar_sar.mp4'\n",
        "# CAMERA_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/camera.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqhiiwnSHaB8"
      },
      "outputs": [],
      "source": [
        "def Convert_to_Meters(frame):\n",
        "  center_x = 625\n",
        "  center_y = 624\n",
        "  height, width = frame.shape\n",
        "\n",
        "  points = []\n",
        "\n",
        "  for v in range(0, width):\n",
        "        for u in range(0, height):\n",
        "          if frame[u][v]<200:\n",
        "            x = (v-center_x)*0.04\n",
        "            y = -(u-center_y)*0.04\n",
        "            points.append([x,y])\n",
        "  return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlWuU-x-zXSF"
      },
      "outputs": [],
      "source": [
        "def GetThreshold_Binary(frame):\n",
        "  gray_scale = 255 - cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
        "  threshold = 0.9*np.max(gray_scale)\n",
        "  _, thres = cv.threshold(gray_scale, threshold, 255,cv.THRESH_BINARY)\n",
        "  point_cloud = Convert_to_Meters(thres)\n",
        "  point_cloud = np.array(point_cloud)\n",
        "\n",
        "  # fig = plt.figure(figsize=(12,6))\n",
        "  # plt.scatter(point_cloud[:,0],point_cloud[:,1],s=0.1)\n",
        "  # plt.xlabel('Z')\n",
        "  # plt.ylabel('X')\n",
        "  # plt.xlim(-25, 25)\n",
        "  # plt.ylim(0, 25)\n",
        "\n",
        "  # plt.savefig('saved_figure.jpg')\n",
        "  # im = cv.imread('saved_figure.jpg')\n",
        "  # frames.append(im)\n",
        "  # plt.close()\n",
        "\n",
        "  return thres, point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o--rPYmC54_a"
      },
      "outputs": [],
      "source": [
        "def Plot_Camera_with_SAR(point_cloud_SAR, point_cloud_Camera):\n",
        "\n",
        "  fig = plt.figure(figsize=(12,6))\n",
        "  plt.scatter(point_cloud_SAR[:,0],point_cloud_SAR[:,1],s=0.1, color = 'black')\n",
        "\n",
        "  if len(point_cloud_Camera) > 0:\n",
        "    color = (point_cloud_Camera[:, 3])\n",
        "    color_plot = np.array([COLOR_MAP[int(c)] for c in color])\n",
        "    plt.scatter(point_cloud_Camera[:,0],point_cloud_Camera[:,2],s=0.1 , color = color_plot/255)\n",
        "  plt.xlabel('Z')\n",
        "  plt.ylabel('X')\n",
        "  plt.xlim(-25, 25)\n",
        "  plt.ylim(0, 25)\n",
        "\n",
        "  plt.savefig('saved_figure.jpg')\n",
        "  im = cv.imread('saved_figure.jpg')\n",
        "  frames.append(im)\n",
        "  plt.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPgKg5mNzpBh"
      },
      "outputs": [],
      "source": [
        "frames=[]\n",
        "video_radar = cv.VideoCapture(RADAR_VIDEO)\n",
        "\n",
        "radar_thresholded = np.zeros((num_frames,624,1250))\n",
        "\n",
        "SAR_points = []\n",
        "\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video_radar.read()\n",
        "        if not _: break\n",
        "        radar_thresholded[i], points_SAR = GetThreshold_Binary(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Adaptive(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Gaussian(frame)\n",
        "\n",
        "        points_camera = np.array(pc3darray_SAR_frame[i])\n",
        "        Plot_Camera_with_SAR(points_SAR, points_camera)\n",
        "        SAR_points.append(points_SAR)\n",
        "\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "\n",
        "height, width, layers = frames[0].shape\n",
        "size = (width,height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv.VideoWriter('SARandCamera_pointcloud.avi', fourcc, 30.0, size)\n",
        "\n",
        "for i in range(len(frames)):\n",
        "    out.write(frames[i])\n",
        "out.release()\n",
        "\n",
        "\n",
        "SAR_points = np.array(radar_thresholded)\n",
        "Camera_points = np.array(pc3darray_SAR_frame)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sensor_points.npy', 'wb') as f:\n",
        "    np.save(f, SAR_points, allow_pickle= True)\n",
        "    np.save(f, Camera_points, allow_pickle = True)"
      ],
      "metadata": {
        "id": "Zklbd74lW8ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp sensor_points.npy /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "tT6y18w_cis9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman Filtering"
      ],
      "metadata": {
        "id": "oBi1_9momOAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "os.chdir(\"drive/MyDrive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25QRuGIlmSU1",
        "outputId": "befc12a6-6c93-48b3-93fe-4570072f522c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sensor_points_100_frames.npy', 'rb') as f:\n",
        "    sar_points = np.load(f, allow_pickle= True)\n",
        "    camera_points = np.load(f, allow_pickle= True)\n",
        "\n"
      ],
      "metadata": {
        "id": "uNqsFZZbYRBL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occupancy_radar_points = sar_points.copy()\n",
        "x_min = 1000\n",
        "z_min = 1000\n",
        "x_max = -1000\n",
        "z_max = -1000\n",
        "print(\"k\",len(sar_points[0]))\n",
        "print(len(sar_points[0][0]))\n",
        "for i in range(len(camera_points)):\n",
        "  occupancy_map = {}\n",
        "  for j in range(len(camera_points[0])):\n",
        "    x = camera_points[i][j][0]\n",
        "    z = camera_points[i][j][2]\n",
        "\n",
        "    x_index = int(round(x/.04)) + 625\n",
        "    z_index = int(round(z/.04))\n",
        "    x_min = min(x_index,x_min)\n",
        "    z_min = min(z_index,z_min)\n",
        "    x_max = max(x_index,x_max)\n",
        "    z_max = max(z_index,z_max)\n",
        "\n",
        "    # print(x_index,z_index)\n",
        "    occupancy_map[(z_index,x_index)] = 1\n",
        "\n",
        "  for k in range(len(sar_points[0])):\n",
        "    for l in range(len(sar_points[0][0])):\n",
        "      # print(k,l)\n",
        "      # print(l,k)\n",
        "      if ((k,l) in occupancy_map):\n",
        "          occupancy_radar_points[i][k][l] = 1\n",
        "      else:\n",
        "          occupancy_radar_points[i][k][l] = 0\n",
        "print(x_min,x_max)\n",
        "print(z_min,z_max)\n"
      ],
      "metadata": {
        "id": "PbmpoMd4TtyS",
        "outputId": "90870da5-777f-42c1-fa4a-441d78aa2068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k 624\n",
            "1250\n",
            "135 714\n",
            "123 866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(occupancy_radar_points[34])\n",
        "print(occupancy_map)\n",
        "print(np.count_nonzero(occupancy_radar_points))"
      ],
      "metadata": {
        "id": "mxxcCaH3eHa4",
        "outputId": "248c33cc-ea62-49b4-bf4a-ef762a350776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "{(341, 329): 1, (340, 330): 1, (338, 332): 1, (336, 334): 1, (334, 335): 1, (331, 338): 1, (327, 341): 1, (321, 346): 1, (315, 351): 1, (305, 360): 1, (297, 366): 1, (290, 372): 1, (281, 380): 1, (267, 391): 1, (253, 402): 1, (247, 408): 1, (240, 413): 1, (235, 418): 1, (231, 421): 1, (229, 423): 1, (227, 425): 1, (225, 426): 1, (223, 428): 1, (220, 430): 1, (218, 432): 1, (216, 434): 1, (214, 435): 1, (212, 437): 1, (211, 438): 1, (210, 439): 1, (208, 440): 1, (208, 441): 1, (207, 441): 1, (206, 442): 1, (204, 443): 1, (204, 444): 1, (203, 445): 1, (202, 445): 1, (201, 446): 1, (200, 447): 1, (199, 448): 1, (198, 448): 1, (197, 449): 1, (196, 450): 1, (195, 451): 1, (193, 452): 1, (348, 326): 1, (341, 331): 1, (339, 333): 1, (338, 334): 1, (335, 336): 1, (333, 337): 1, (331, 340): 1, (327, 343): 1, (321, 348): 1, (312, 355): 1, (303, 362): 1, (293, 371): 1, (282, 380): 1, (269, 391): 1, (259, 399): 1, (251, 406): 1, (246, 410): 1, (240, 415): 1, (235, 419): 1, (231, 423): 1, (228, 424): 1, (227, 426): 1, (225, 427): 1, (223, 429): 1, (220, 431): 1, (217, 434): 1, (215, 435): 1, (214, 437): 1, (212, 438): 1, (211, 439): 1, (210, 440): 1, (209, 440): 1, (207, 442): 1, (206, 443): 1, (205, 444): 1, (204, 445): 1, (203, 446): 1, (202, 446): 1, (201, 447): 1, (200, 448): 1, (199, 449): 1, (198, 450): 1, (197, 451): 1, (195, 452): 1, (194, 453): 1, (348, 327): 1, (342, 332): 1, (341, 333): 1, (339, 335): 1, (335, 338): 1, (332, 341): 1, (328, 343): 1, (322, 348): 1, (312, 357): 1, (296, 370): 1, (276, 387): 1, (263, 397): 1, (257, 402): 1, (253, 406): 1, (249, 409): 1, (245, 412): 1, (240, 416): 1, (235, 420): 1, (229, 425): 1, (228, 426): 1, (225, 428): 1, (223, 430): 1, (220, 433): 1, (218, 434): 1, (216, 436): 1, (214, 438): 1, (212, 439): 1, (211, 440): 1, (210, 441): 1, (209, 442): 1, (208, 443): 1, (207, 443): 1, (206, 444): 1, (205, 445): 1, (204, 446): 1, (202, 447): 1, (201, 448): 1, (200, 449): 1, (196, 452): 1, (348, 329): 1, (342, 334): 1, (341, 335): 1, (339, 336): 1, (335, 340): 1, (333, 342): 1, (329, 344): 1, (323, 349): 1, (307, 362): 1, (273, 390): 1, (261, 400): 1, (256, 404): 1, (253, 407): 1, (251, 409): 1, (247, 412): 1, (244, 415): 1, (239, 418): 1, (235, 422): 1, (232, 424): 1, (230, 426): 1, (228, 427): 1, (225, 429): 1, (223, 432): 1, (220, 434): 1, (218, 436): 1, (216, 437): 1, (214, 439): 1, (212, 440): 1, (211, 441): 1, (210, 442): 1, (209, 443): 1, (208, 444): 1, (207, 444): 1, (206, 445): 1, (205, 446): 1, (204, 447): 1, (203, 447): 1, (202, 448): 1, (202, 449): 1, (201, 449): 1, (200, 450): 1, (198, 451): 1, (197, 452): 1, (348, 332): 1, (343, 336): 1, (341, 336): 1, (340, 337): 1, (335, 341): 1, (333, 343): 1, (330, 345): 1, (324, 351): 1, (289, 379): 1, (262, 401): 1, (257, 405): 1, (255, 407): 1, (252, 409): 1, (250, 411): 1, (247, 413): 1, (243, 416): 1, (238, 420): 1, (235, 423): 1, (232, 426): 1, (230, 427): 1, (228, 429): 1, (225, 431): 1, (223, 433): 1, (220, 435): 1, (218, 437): 1, (216, 438): 1, (214, 440): 1, (212, 441): 1, (211, 442): 1, (210, 443): 1, (209, 444): 1, (208, 445): 1, (207, 445): 1, (206, 446): 1, (205, 447): 1, (204, 448): 1, (203, 449): 1, (201, 450): 1, (200, 451): 1, (199, 452): 1, (197, 453): 1, (342, 338): 1, (341, 338): 1, (334, 344): 1, (332, 346): 1, (323, 353): 1, (276, 391): 1, (258, 406): 1, (256, 407): 1, (254, 409): 1, (251, 411): 1, (249, 413): 1, (246, 415): 1, (242, 418): 1, (238, 421): 1, (235, 424): 1, (232, 427): 1, (230, 428): 1, (228, 430): 1, (225, 432): 1, (223, 434): 1, (220, 436): 1, (218, 438): 1, (216, 440): 1, (214, 441): 1, (213, 442): 1, (211, 443): 1, (210, 444): 1, (209, 445): 1, (208, 446): 1, (207, 446): 1, (206, 447): 1, (206, 448): 1, (205, 448): 1, (204, 449): 1, (202, 450): 1, (202, 451): 1, (201, 451): 1, (200, 452): 1, (335, 345): 1, (334, 346): 1, (318, 359): 1, (267, 399): 1, (256, 409): 1, (254, 410): 1, (252, 411): 1, (251, 413): 1, (249, 415): 1, (245, 417): 1, (242, 420): 1, (238, 423): 1, (235, 426): 1, (232, 428): 1, (230, 429): 1, (228, 431): 1, (226, 433): 1, (223, 435): 1, (220, 437): 1, (218, 439): 1, (216, 441): 1, (214, 442): 1, (213, 443): 1, (211, 444): 1, (210, 445): 1, (209, 446): 1, (208, 447): 1, (207, 447): 1, (207, 448): 1, (205, 449): 1, (205, 450): 1, (204, 450): 1, (203, 451): 1, (202, 452): 1, (201, 452): 1, (200, 453): 1, (335, 347): 1, (312, 365): 1, (265, 403): 1, (255, 411): 1, (254, 412): 1, (252, 413): 1, (250, 414): 1, (248, 416): 1, (245, 418): 1, (242, 421): 1, (238, 424): 1, (235, 427): 1, (232, 429): 1, (231, 430): 1, (229, 431): 1, (226, 434): 1, (223, 436): 1, (220, 438): 1, (218, 440): 1, (215, 443): 1, (213, 444): 1, (212, 445): 1, (210, 446): 1, (209, 447): 1, (208, 448): 1, (207, 449): 1, (206, 450): 1, (204, 451): 1, (203, 452): 1, (202, 453): 1, (201, 453): 1, (200, 454): 1, (336, 347): 1, (306, 372): 1, (262, 407): 1, (255, 412): 1, (253, 414): 1, (251, 415): 1, (250, 416): 1, (248, 418): 1, (245, 420): 1, (242, 423): 1, (238, 425): 1, (235, 428): 1, (233, 430): 1, (231, 431): 1, (229, 432): 1, (226, 435): 1, (223, 437): 1, (221, 439): 1, (219, 441): 1, (217, 442): 1, (214, 445): 1, (212, 446): 1, (211, 447): 1, (210, 447): 1, (209, 448): 1, (208, 449): 1, (207, 450): 1, (206, 451): 1, (205, 451): 1, (204, 452): 1, (203, 453): 1, (202, 454): 1, (201, 455): 1, (336, 350): 1, (303, 376): 1, (260, 410): 1, (254, 414): 1, (252, 416): 1, (251, 417): 1, (249, 418): 1, (247, 420): 1, (244, 422): 1, (241, 424): 1, (238, 427): 1, (235, 429): 1, (233, 431): 1, (231, 432): 1, (229, 434): 1, (226, 436): 1, (224, 438): 1, (221, 440): 1, (219, 442): 1, (217, 443): 1, (216, 444): 1, (213, 447): 1, (212, 447): 1, (211, 448): 1, (210, 449): 1, (209, 449): 1, (209, 450): 1, (208, 450): 1, (207, 451): 1, (206, 452): 1, (205, 453): 1, (204, 453): 1, (203, 454): 1, (202, 455): 1, (335, 352): 1, (300, 380): 1, (258, 412): 1, (254, 416): 1, (251, 418): 1, (250, 419): 1, (249, 420): 1, (246, 421): 1, (243, 424): 1, (241, 426): 1, (238, 428): 1, (235, 430): 1, (233, 432): 1, (231, 433): 1, (229, 435): 1, (226, 437): 1, (224, 439): 1, (221, 441): 1, (219, 443): 1, (217, 444): 1, (216, 445): 1, (214, 446): 1, (212, 448): 1, (211, 449): 1, (210, 450): 1, (208, 451): 1, (207, 452): 1, (206, 453): 1, (205, 454): 1, (204, 454): 1, (203, 455): 1, (335, 354): 1, (296, 384): 1, (258, 414): 1, (253, 417): 1, (251, 419): 1, (250, 420): 1, (248, 421): 1, (246, 423): 1, (243, 425): 1, (241, 427): 1, (238, 430): 1, (235, 431): 1, (233, 433): 1, (231, 435): 1, (229, 436): 1, (226, 438): 1, (224, 440): 1, (222, 442): 1, (219, 444): 1, (218, 445): 1, (216, 446): 1, (215, 447): 1, (213, 448): 1, (212, 449): 1, (212, 450): 1, (211, 450): 1, (210, 451): 1, (209, 451): 1, (208, 452): 1, (208, 453): 1, (207, 453): 1, (206, 454): 1, (205, 455): 1, (203, 456): 1, (335, 356): 1, (293, 388): 1, (257, 416): 1, (253, 419): 1, (251, 421): 1, (249, 422): 1, (247, 424): 1, (245, 425): 1, (242, 427): 1, (240, 429): 1, (237, 431): 1, (235, 433): 1, (233, 435): 1, (231, 436): 1, (229, 438): 1, (226, 440): 1, (224, 441): 1, (222, 443): 1, (220, 445): 1, (218, 446): 1, (216, 447): 1, (215, 448): 1, (214, 449): 1, (213, 450): 1, (211, 451): 1, (210, 452): 1, (209, 453): 1, (208, 454): 1, (207, 454): 1, (206, 455): 1, (204, 456): 1, (334, 358): 1, (291, 391): 1, (257, 418): 1, (253, 421): 1, (251, 422): 1, (249, 423): 1, (247, 425): 1, (245, 427): 1, (242, 429): 1, (240, 430): 1, (237, 433): 1, (235, 434): 1, (233, 436): 1, (231, 437): 1, (229, 439): 1, (226, 441): 1, (224, 442): 1, (222, 444): 1, (220, 446): 1, (218, 447): 1, (217, 448): 1, (215, 449): 1, (214, 450): 1, (213, 451): 1, (212, 452): 1, (211, 452): 1, (210, 453): 1, (209, 454): 1, (208, 455): 1, (207, 455): 1, (206, 456): 1, (205, 457): 1, (333, 361): 1, (289, 394): 1, (256, 419): 1, (252, 422): 1, (250, 424): 1, (248, 425): 1, (246, 427): 1, (244, 428): 1, (242, 430): 1, (240, 432): 1, (237, 434): 1, (235, 435): 1, (233, 437): 1, (231, 438): 1, (229, 440): 1, (227, 442): 1, (224, 443): 1, (222, 445): 1, (219, 448): 1, (217, 449): 1, (216, 450): 1, (215, 451): 1, (214, 451): 1, (213, 452): 1, (212, 453): 1, (211, 453): 1, (211, 454): 1, (210, 454): 1, (209, 455): 1, (208, 456): 1, (207, 456): 1, (205, 458): 1, (332, 363): 1, (285, 399): 1, (255, 421): 1, (252, 424): 1, (250, 426): 1, (248, 427): 1, (246, 429): 1, (244, 430): 1, (242, 432): 1, (239, 433): 1, (237, 435): 1, (235, 436): 1, (234, 438): 1, (231, 439): 1, (229, 441): 1, (227, 443): 1, (225, 444): 1, (223, 446): 1, (221, 447): 1, (219, 449): 1, (217, 450): 1, (216, 451): 1, (215, 452): 1, (214, 452): 1, (213, 453): 1, (212, 454): 1, (211, 455): 1, (210, 455): 1, (210, 456): 1, (209, 456): 1, (207, 457): 1, (331, 366): 1, (284, 401): 1, (256, 422): 1, (252, 425): 1, (250, 427): 1, (248, 428): 1, (246, 430): 1, (244, 431): 1, (242, 433): 1, (240, 434): 1, (237, 436): 1, (236, 437): 1, (234, 439): 1, (232, 440): 1, (229, 442): 1, (227, 444): 1, (225, 445): 1, (223, 447): 1, (221, 448): 1, (218, 451): 1, (217, 451): 1, (216, 452): 1, (215, 453): 1, (214, 453): 1, (214, 454): 1, (213, 454): 1, (212, 455): 1, (211, 456): 1, (210, 457): 1, (209, 457): 1, (208, 458): 1, (330, 368): 1, (285, 402): 1, (257, 423): 1, (252, 427): 1, (251, 427): 1, (249, 429): 1, (247, 431): 1, (245, 432): 1, (242, 434): 1, (240, 435): 1, (238, 437): 1, (237, 438): 1, (235, 439): 1, (232, 441): 1, (230, 443): 1, (228, 444): 1, (225, 446): 1, (223, 448): 1, (222, 449): 1, (220, 450): 1, (217, 452): 1, (216, 453): 1, (215, 454): 1, (214, 455): 1, (213, 455): 1, (213, 456): 1, (212, 456): 1, (211, 457): 1, (209, 458): 1, (329, 371): 1, (285, 404): 1, (256, 425): 1, (252, 428): 1, (250, 429): 1, (249, 430): 1, (247, 432): 1, (245, 433): 1, (243, 435): 1, (240, 436): 1, (238, 438): 1, (237, 439): 1, (235, 440): 1, (233, 442): 1, (230, 444): 1, (228, 445): 1, (226, 447): 1, (224, 449): 1, (222, 450): 1, (221, 451): 1, (219, 452): 1, (218, 453): 1, (217, 453): 1, (216, 454): 1, (215, 455): 1, (214, 456): 1, (213, 457): 1, (212, 457): 1, (211, 458): 1, (210, 458): 1, (356, 353): 1, (355, 354): 1, (354, 355): 1, (348, 359): 1, (347, 360): 1, (346, 360): 1, (332, 371): 1, (329, 373): 1, (286, 404): 1, (257, 426): 1, (251, 430): 1, (250, 431): 1, (249, 432): 1, (247, 433): 1, (245, 434): 1, (243, 436): 1, (241, 438): 1, (239, 439): 1, (237, 440): 1, (235, 441): 1, (233, 443): 1, (230, 445): 1, (228, 446): 1, (226, 448): 1, (224, 450): 1, (223, 451): 1, (221, 452): 1, (220, 453): 1, (219, 454): 1, (218, 454): 1, (217, 455): 1, (216, 455): 1, (216, 456): 1, (215, 456): 1, (214, 457): 1, (212, 458): 1, (211, 459): 1, (355, 356): 1, (347, 361): 1, (346, 362): 1, (332, 372): 1, (329, 374): 1, (288, 404): 1, (258, 426): 1, (251, 431): 1, (250, 432): 1, (249, 433): 1, (248, 434): 1, (246, 435): 1, (244, 437): 1, (239, 440): 1, (238, 441): 1, (236, 442): 1, (233, 444): 1, (231, 446): 1, (229, 447): 1, (226, 449): 1, (222, 452): 1, (218, 455): 1, (217, 456): 1, (215, 457): 1, (214, 458): 1, (213, 459): 1, (212, 459): 1, (329, 376): 1, (292, 403): 1, (260, 426): 1, (252, 432): 1, (251, 433): 1, (249, 434): 1, (248, 435): 1, (246, 436): 1, (244, 438): 1, (242, 439): 1, (240, 441): 1, (238, 442): 1, (236, 443): 1, (233, 445): 1, (231, 447): 1, (229, 448): 1, (227, 450): 1, (225, 451): 1, (224, 452): 1, (222, 453): 1, (221, 454): 1, (220, 455): 1, (219, 455): 1, (219, 456): 1, (218, 456): 1, (217, 457): 1, (216, 458): 1, (215, 458): 1, (215, 459): 1, (214, 459): 1, (213, 460): 1, (329, 378): 1, (296, 402): 1, (262, 426): 1, (253, 432): 1, (251, 434): 1, (250, 435): 1, (248, 436): 1, (247, 437): 1, (244, 439): 1, (242, 440): 1, (240, 442): 1, (238, 443): 1, (236, 444): 1, (234, 446): 1, (232, 448): 1, (229, 449): 1, (227, 451): 1, (225, 452): 1, (224, 453): 1, (223, 454): 1, (221, 455): 1, (220, 456): 1, (219, 457): 1, (218, 457): 1, (217, 458): 1, (216, 459): 1, (215, 460): 1, (214, 460): 1, (213, 461): 1, (328, 380): 1, (303, 398): 1, (266, 425): 1, (255, 433): 1, (253, 434): 1, (251, 435): 1, (249, 437): 1, (247, 438): 1, (245, 440): 1, (243, 441): 1, (241, 443): 1, (239, 444): 1, (237, 445): 1, (234, 447): 1, (232, 449): 1, (230, 450): 1, (228, 452): 1, (226, 453): 1, (225, 454): 1, (223, 455): 1, (222, 456): 1, (221, 456): 1, (220, 457): 1, (219, 458): 1, (218, 459): 1, (217, 459): 1, (216, 460): 1, (214, 461): 1, (348, 368): 1, (340, 374): 1, (328, 382): 1, (308, 396): 1, (271, 423): 1, (256, 433): 1, (254, 434): 1, (252, 436): 1, (250, 437): 1, (248, 439): 1, (245, 441): 1, (243, 442): 1, (240, 444): 1, (237, 446): 1, (235, 448): 1, (233, 449): 1, (230, 451): 1, (228, 453): 1, (226, 454): 1, (225, 455): 1, (224, 455): 1, (223, 456): 1, (222, 457): 1, (221, 458): 1, (220, 458): 1, (219, 459): 1, (218, 460): 1, (217, 460): 1, (216, 461): 1, (215, 462): 1, (348, 370): 1, (340, 375): 1, (339, 376): 1, (313, 394): 1, (278, 419): 1, (258, 433): 1, (255, 435): 1, (253, 436): 1, (251, 438): 1, (249, 439): 1, (246, 441): 1, (244, 443): 1, (242, 444): 1, (240, 445): 1, (238, 447): 1, (236, 449): 1, (233, 450): 1, (231, 452): 1, (229, 454): 1, (227, 455): 1, (226, 456): 1, (225, 456): 1, (223, 457): 1, (222, 458): 1, (221, 459): 1, (220, 459): 1}\n",
            "91793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(occupancy_radar_points[50], interpolation='nearest', origin='upper')\n"
      ],
      "metadata": {
        "id": "6KOzUdkbb1Vx",
        "outputId": "192ff5d9-1c45-49d0-b3a9-0de3e453d209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ca38414b760>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(sar_points[50], interpolation='nearest', origin='upper')\n",
        "print(sar_points[44][582][800])\n",
        "# ax.imshow(sar_points[32], interpolation='nearest', origin='upper')\n"
      ],
      "metadata": {
        "id": "kCIa8m10YMdk",
        "outputId": "db7dd069-2f62-41c3-93c5-e92e7a78876f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAEoCAYAAABy5QoYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGFklEQVR4nO2df3gV1Z3/30BIADGJoMklFZRuUUjFoqAQtW63Zo2K2yqo1SciZVl9ZIMVUIrZKu3aalh9tlW7CuqzKz6PIi3PI25lBZYCYpUIGIXyQ1BXNCjcYEuTgBUIZL5/+L3XucM5Z86ZO/fOj/t+Pc99IDNnZs6ce+bM+3zO5/O5PSzLskAIIYQQEiJ6Bl0BQgghhBAnFCiEEEIICR0UKIQQQggJHRQohBBCCAkdFCiEEEIICR0UKIQQQggJHRQohBBCCAkdFCiEEEIICR0UKIQQQggJHRQohBBCCAkdgQqUxx9/HGeeeSb69OmDsWPHYuPGjUFWhxBCCCEhITCB8pvf/AazZs3CT3/6U7z99tv41re+hbq6Ouzfvz+oKhFCCCEkJPQI6scCx44diwsuuAD/8R//AQDo7u7G4MGDcccdd+Cee+4JokqEEEIICQlFQVz06NGjaGlpQWNjY3pbz549UVtbi+bm5hPKHzlyBEeOHEn/3d3djQMHDmDgwIHo0aNHXupMCCGEkOywLAsHDx5EVVUVevZUL+IEIlD+9Kc/4fjx46isrMzYXllZiZ07d55QvqmpCf/6r/+ar+oRQgghJIfs2bMHp59+urJMIALFlMbGRsyaNSv9d0dHB4YMGYKP3z4Tpf0ZiBR3rj1rJJa+t9WX47M9FyFBEqf+e+1ZIzP+Vt1Xqmxc7r2Q6TzUjTPO/wgnn3yya9lABMqpp56KXr16oa2tLWN7W1sbEonECeVLSkpQUlJywvbS/j1RejIFStxZvW87svHnth+f7bncqKsaBQBYuXdzzq5BCpO6qlEo6gGtMS8K/bCoR28A9jrK7+vL51ZdhoSXuqpRJ/RFHfeMQJ1kL7zwQvz6178G8KVfyZAhQzB9+nRXJ9nOzk6UlZXhL+99nQIl5Ig6ZpDXTg3cTrzUMch7IyQKIkSHuNwH0aPzYDdOOetDdHR0oLS0VFk2sCWeWbNmYfLkyRgzZgwuvPBCPPLII/j8888xZcqUoKpEckCQg47s2iv3bvZlUAzLgOqn6CLRIfX9Rk0o2+ub77rLnnuKpHASmED5wQ9+gM8++wxz585FMpnEqFGjsGLFihMcZ7PBS+eP2sMeFcIwANi/22zqEZZ7ESEaeNmf44HzuwxDP/SKrP/mGlFb2duRz0u4CGyJJxu4xBMtojyQhgVnGzoHeLZt/JH1gSh+9ykhYO/HQQv+KLZjFInEEg+JP7KHnoOBOc72YvsVJmF6qWeDbJnVbWzI1dgR1XaMOzQ/kJwhWk4JyrRLSNSxP0+6zt9hRbVEqboXt/0kXnCJh+SFfJqjZdE7YZ0lhbluJPyIXthh7086/jRRXsIicrjEQ0KFcwDN9Qs57ANaIfiPxGEZIszIXt5RaOtU3e33EIV6k/xDCwrJC2HLhxJUPQD/ogV0z8GZaLRx+/6iKAZlddbJXaR7jzIrjck5iP+YWFAoUAjJIUHleQD8y/dCwoFbDo8UUfmuvfRNr8+Tc3KQIiptFScoULIgLLNtkjuCTBSVCyhCCgs3J9Io4EUkmEb9iMKyo+SbFlcoUEhkCMMAEdUEWCZmcpI/8tV/VMshYf3+s8l/4mXJhtaS8EGBQkJDUC9Llb9HIb7A3UIzC609ckUQPzsQtf6cbwsmRUq4oEAhBYXbzCqsJnHd2a7XWbHMN0FnOwfy7GF7nogfPlJ++KGQ4KBAIQWDzGSsm+xJZRqO0kAmEh2q/BgcrMNP1PqgLqq+J3Jm9asN7GNFHNs1KlCgEF8I84Osmp3q7NN1rIsa/M2ecBL1fuUXun5TqoilQm/DqMNEbcQXwjwQqOomG/jiajXRbQviP7p9Jk5RY35jT9xmT+fv17nZ9tGFFhQSa3Re4rkexLI5Z1xm3qa/nxLV+41i2vlco7Lq5aptRE7yFCjhgEs8JHDCOhh4zeYapvsxyefgxSE2LqIoCEQWO4Bt6bbUmAvHV9XEI0zPc6FBgaIBO2j+CUubx+0FrBr8GV5cWITlGVMhcmh3s3CmypLoQ4FCyP8n7jNa3aWTuNwvKUz8EF5REG+FgIlA4dudRBoTC8HKvZvTn7qqUcZ+EX7XzfRc9vPJ6q9yABZ9VOdSXd+5j5yIqJ3jjOo+nW3hdo5coGOlIeGCAoVEGp0ZkenA6NdglYvZms696F7XHjFhF20y4UNfFTNES226QjOK6KSbV21TlRH1T5O241JnNOESD4klosHN/v+wD0iihFX5QJWRV7RddqxbWZN6RA2dZUXdvB9xQNVf/HJe1T1PFJ79uEMfFFLwuA2Kzm35qE8213EL1VT9LUKWyE1V3nRJKRui/hIRfV9uFqhCeHnmQjiIonVU54tyRE+U6iqDAiVHxKFzFAKF4DhqH4j9RHXOKLdXELiJyEJCdO+F3B6FDJ1kcwQfovCj+3KN8ncpEspe7idObRJGVMticfE7UTnE2nG2RaoPi3ybvFzTC3H5DuIMLSgFSlStQTJTuepl4GZpyIdfSjZr64BeYisvA24U+0AciOrz50TXd8ZtidLLdePQfoUIl3hIpDFZP7Zvi5tDpu4grHPfzhdDWO+ZxBeVD5OJ0GHfjTYUKCS0uIkPQD902A8Lgtv1wvhCl5nPOXiTXOAWEefEbpF0i2gS7dd1YGV/jyb0Qfn/cI0xfKgGFJ31aDfBINrudk03ghwE7bkf3Nb1iTeCykUSlRwobr5KTl8SWf4S0fPtFDwy7LlPZPUg8aMo6ArkEnbgeGEfnFThijroLgnlwzohmpHqhu+myrGvn4iu9SvXvkdeyoTp+3Q+d6L/ux0nOlYnD4wfocdRJ073YkqsLSimRGE2EzdMB3CZQ6uJdcHEX0U2y/Wjr6hm0LJ9XiMfCpFUO6msULLtprhFj8m+M5nADNNYZK+bs/+pBIXsfp1toXPvzuu5tU+cno843Yspxj4or732Gh5++GG0tLRg3759WLp0Ka655pr0fsuy8NOf/hRPP/002tvbcfHFF2P+/PkYNmxYusyBAwdwxx134OWXX0bPnj0xceJEPProo+jfv79WHeiDEl+8zBb8zHuiEjq5hvlHgkPne3fzkVAtP7hFmqWOCdtsWRZ9o7J+qNpEx1oo8l/J9h7C1KaFTk6dZJcvX4433ngDo0ePxoQJE04QKP/2b/+GpqYmPPvssxg6dCjuu+8+bN26FTt27ECfPn0AAFdeeSX27duHJ598El1dXZgyZQouuOACLFq0SO8GYyRQ+PB4R9dR1iS0McjvI4wOuXFEtx/Ykb2gVeLY7SUuK5tPdPq7rlO2U5io/q+KxnOrp6mA4XMVLvIWxdOjR48MgWJZFqqqqnDXXXfh7rvvBgB0dHSgsrISCxcuxI033oh3330X1dXV2LRpE8aMGQMAWLFiBa666ip88sknqKqqcr/BGAkU4g23wUk2eLpFEuQLOvvFAy9LMaIlxrD3ATfrnuy5kpV3u5bMghVGKxMxI7Aont27dyOZTKK2tja9raysDGPHjkVzczMAoLm5GeXl5WlxAgC1tbXo2bMnNmzYIDzvkSNH0NnZmfEhhY3pAKXyOwlKnNCXJJqIXsYm36Op31HYcIuKs0ftZHN+mVNtNs9MFNqXfIWvAiWZTAIAKisrM7ZXVlam9yWTSVRUVGTsLyoqwoABA9JlnDQ1NaGsrCz9GTx4sJ/VJjHEPsiJBv58CAM3R9dUGdWxUXlpFRIm0WMm/UzXAdQvdPtWShyInGVVZU2jfJx1kjnSZtM+Jg7yJHgiEWbc2NiIWbNmpf/u7OykSCFSRMs5KXTW27MVLybnkK3hi/aHhXyJPb9eJLluQ9XL0yRazHmuXOP2snZaMOzHyOqp2q4riNx8U1TCSAcdHxoSDnwVKIlEAgDQ1taGQYMGpbe3tbVh1KhR6TL79+/POO7YsWM4cOBA+ngnJSUlKCkp8bOqJMYEPch4NUW7DcL5RvbSzFfdwtAGgFo86DrJ6pRX7TdxKDVB10lYdF2VcJD5fKmigETX13HCVd2Hqv6yfk2hEh58FShDhw5FIpHA6tWr04Kks7MTGzZswLRp0wAANTU1aG9vR0tLC0aPHg0AWLNmDbq7uzF27Fg/q0MKHJlTrNsxpnhxeI3CIBj2+uUat7Bhne0qa54MnUgWLz4vTuuHbhSc7Hxu+0U+JG6WFzcLlFv9VWVT13GzVhV6vw8TxgLl0KFD+OCDD9J/7969G5s3b8aAAQMwZMgQzJgxA7/4xS8wbNiwdJhxVVVVOtJnxIgRuOKKK3DrrbdiwYIF6OrqwvTp03HjjTdqRfAQYoLz5eCXMMjWaY+DoDv59hXQeWE5X3RudXRzKPWjfibHqpY3nPtF59C9f5koE4kG0fWd/9e9D1FZp0jUEYAkHBiHGb/66qv4u7/7uxO2T548GQsXLkwnanvqqafQ3t6OSy65BE888QTOOuusdNkDBw5g+vTpGYnaHnvsMSZqI1njddZE4o2f/iy5EE757peqUN5UfUS+Rro+JKZLX7Jre1nasgsSXQsLyR/8NWNCIJ45RXVQUtXbyxIT+Qo/BIdfwiUX35+b34fKzyPXiJ5P3XKmExHR8lBUx4MoYyJQIhHFQ4gMNxOvV2e6fCPzB1C9VNzW9YkebrNsHbJ9mYuWTPIhVlTOsLqYiDPndUVWGlUdVMs+bo61queLgiWc0IJCIo+bdUE2YIUFN3O7iLDdQyHhxfFVl1x+rzIHU5kFwi2iKJv71/Ehcbu+vZzpMq5KGJHcwiUeQrIgaBGTjeMlCQYTC4KJH4dfDt2pc6b+lp1ftSyaqyUsXWEhuw/dc6gskjrXJ/4QWKp7QuJAEA6LIpO3sx5czgkvqe9G9yXrRjZiQPTidfPF0o10y7b/ud2XKLLHeW0Ty4rb82RSN5J/6INCYk1UHGRl6+RhrS+RY7pkocJL9Jmbg6jJdVXbTX1PnGXdLCIyfxNd3xRnedFSL5+vcEMLCok1+YxI8AoHycLAaWXR/d699A9VfzdZGlFZIHSWSNyWdEQO7G5LSqbLTSIhInJKVllvSDDQB4XEFi+zT0KCwiRc1o6pZUQnPDdXPjXOv1VRarJ6q7bp7tex1pDcQB8UUtCoZkaEhBWnrwhwYl+2l7OX0cFumfASXmwaaSYqp3sOXetM6l7sH906cgITfmhBIbHEGamgWu8mJOyorAv2MrohtiKLhvM6TvyM5JH5kfhlFbFvk12DY0Aw0IJCCh5ZeGRqlsWBiUQJu9XEjwgfp5OolzBdL6j8b2ROtM46q9C1NtGqGg0oUEgsUTnYUZyQqCLrv7q5SmQWBje/F2d52fl0kZ1HdA+6TsUyAWcXYyKnXCInaCFHgUJiidtsM+gHjxA/MbEsOJc5TZ1PRedVldcVFnZx4nRiNXHatdfLbRtRE3Q7UaCQWKHKkcDZE4k7oiXN1PbU33ZhYncuzeaZcAtrdvtbFnos26ZClRNFVgcSTihQSKyQ5VWw/0tInHFzBHfbprvc46VO9v/LrplCZDnxYkmxn8ctQoqEC0bxkNghi+BJQesJKRREPiZ2ZM+JbFkkFy90mSCSbcsmEs95vKgeHB9yC6N4SEGjMuVy8CGFhJeol9R2WaSPH3WSCRG3PCZuVhfRMTp/6zrikvxCCwqJPM5Zj2myKUIKCV0rgk6eFD9zo4jqKauXyXMsy4/CsSAYaEEhBYXMBKwqly3ZOAWKynMtnOQLkRNq6l/R8yNbgvFDnDifXacosTvzyuovgxOV6EOBQmKF21q7n9cB1FFDsr/tyEI9Ccklbk6womUXp4Dw4kgry8OicpDVcZ6V1d20fiRccImHRBLnAKqzxEMnOELEiJ4nnaRvJqLfxGFdtbyjOs5+fLbLQCQ3mCzxUKCQWOG2vk5yB9s7uuiI/HxG8cgi8bKJ2iHhwESgFOWpToTkFJEZOJtwxFzgd26JsKEzG9Y5R1i+rzgiWrqRLTE6BUk24kQn4kb03assOm79hP0o+tCCQiKL7gCUbb4Ev+CAScKGajk0td8pZrwsB4mOtV9ftvzjlrvFXkcSDKbtzygeEmvcciXYt+kOoE5HQK8DnsyhT6cejOYh+cQkMka0bGoS9quyxoicWZ3bZBE9JHhyKQ5pQSGRQbVE4jary+cMy21WSkgQqPyzAHk4schhVQevDrciC4moTqolRVpZwgt9UEjkkeVjcCvrRZyojpeJIdU5ORASU/LxApU9T27PikwYmC7rqPbr+KeosNfL/m9qO5/JaEILCokFboNgtudWmbJ1ZnMkeBhl9BUqUaLq56nyXnxOZD4msgmFzvclEzBxd0iPMgwzJnkl6MEgCG/+bMMfSXAUulBxE9qi/bI287p047aUJDreWd7rvZBgyZmTbFNTEy644AKcfPLJqKiowDXXXINdu3ZllDl8+DAaGhowcOBA9O/fHxMnTkRbW1tGmdbWVowfPx79+vVDRUUFZs+ejWPHjplUhYSMXA0Cbk6lom32jx91c4sk4AAYLewOl8CJfSbuTpi6FgqnMHAuoegsdYocY53bnG3uXKLRuRfZ+flsRhsjgbJu3To0NDTgzTffxKpVq9DV1YXLL78cn3/+ebrMzJkz8fLLL2PJkiVYt24d9u7diwkTJqT3Hz9+HOPHj8fRo0exfv16PPvss1i4cCHmzp3r312RvJLLQUDH18Pp/e82+zPBueae7eBnUpe4vyjDgFOsFBLOl7vKD8Qp9kViQHSczrVF22RLQjoU6vcZR7Ja4vnss89QUVGBdevW4dJLL0VHRwdOO+00LFq0CNdddx0AYOfOnRgxYgSam5sxbtw4LF++HFdffTX27t2LyspKAMCCBQswZ84cfPbZZyguLna9Lpd4gidssxS3tWyvYkI3ciCb6+T6XMSMsPXtfCDyFRFZJ9yWZJyIlllUjraicrLoHhJN8pYHpaOjAwAwYMAAAEBLSwu6urpQW1ubLjN8+HAMGTIEzc3NAIDm5maMHDkyLU4AoK6uDp2dndi+fbvwOkeOHEFnZ2fGhwRL2GaedmuK03yf7Tllf7uVt6Mybzv/r3Mtkjv86j/5JpvlKZUV0r6UqXMNURSNzpKrXYzQh4QAWQiU7u5uzJgxAxdffDHOOeccAEAymURxcTHKy8szylZWViKZTKbL2MVJan9qn4impiaUlZWlP4MHD/ZabSIgaoOw7n4/HVhlQsK+TbUv9X/RIE9H23AiW9oLKyK/GtPjdZEJB5GoF/V1+37RRMcpbkz8Ukh88CxQGhoasG3bNixevNjP+ghpbGxER0dH+rNnz56cX7OQiNKL0bSuqtBEHUTiwS4yZBEPsheE7gBNwkdUhIodP+pr7+9uQsFZTucZsG8THWdqySTxwZNAmT59OpYtW4a1a9fi9NNPT29PJBI4evQo2tvbM8q3tbUhkUikyzijelJ/p8o4KSkpQWlpacaHECcyC4WqvNt21eDo9HERRTqo4EAbfuzfa1SFpNOKZ9+ui32JxwTn86AbQaSzXErij5FAsSwL06dPx9KlS7FmzRoMHTo0Y//o0aPRu3dvrF69Or1t165daG1tRU1NDQCgpqYGW7duxf79+9NlVq1ahdLSUlRXV2dzL6TAcA62utYSmYgwWWpRzSBFIkb3RcfBN/yE1ZLiFNbOj9fIG5FFUHep1Wl1EZ1XhspXhRQGRgKloaEBzz33HBYtWoSTTz4ZyWQSyWQSX3zxBQCgrKwMU6dOxaxZs7B27Vq0tLRgypQpqKmpwbhx4wAAl19+OaqrqzFp0iRs2bIFK1euxL333ouGhgaUlJT4f4cktoiEgKoMoB4U3QY+kSBym1nqvsjohxIdwmZJUQkHVb90ihbZcqUTt/t3eyacol23zqTwMAoz7tGjh3D7M888gx/+8IcAvkzUdtddd+GFF17AkSNHUFdXhyeeeCJj+ebjjz/GtGnT8Oqrr+Kkk07C5MmTMW/ePBQV6f00EMOMiQl+RALoRNjIBng3a0mYXnbEnKhFmqiWV0R90ulTYrrUI4vG8dr/o9beJBOmuicFjUxMuIkF1RJRPgZEDrwkX4hEilN8iISFmzCRRd/I6sD+XnjkLQ8KIWFAtfRiR2W+VkUdmCz9mOBcbtIxrxOiQhbmLgtrd1pGnOiGLTufHx1/F9mEgP2epKBAIZFH9mLXcQaUmbtNHWZN0RmEObskpshEudMyIotGcwoZmYjW9VVR7XdCiwpxQoFCYonOYKdy9vPTZ0W0L8phqyT/mIbNq0SFKOzYxMncrT5uVhm347yEM5N4QoFCYoNzUHaL2MnlIOgW5UCICdla8mThw6r9qu2qOjh9WLw8Z0E9IxRG4YIChUQWt3X11N8ywiYUGG5JskVmEXHzL/HrWbBbB91EURgJ25hQ6FCgkEjiHFSdDn+q6APT2aLIJJ4LvPi/kPCTT8dPmbO3qA46z4TXazv7sMx3hRAVFCgksugOqqI8DqoyTnQtMtmgk2eFRBOneM7XNZ19yhn6q2NlMUFmMWF/Jl6hQCGRxZkJ07lPVM45cNutLWE3P5Pok8+XtdOiIbOg5LpOfltqSOFAgUIiiVOcyASJTJg4z2X/N3WcyOk2V3CWSXKFbrRYLvu3znNIiBMKFBIpVOvbbrkYRD4pMiuMc1DnoEqigr0/i0S8yJIici73UzTnwiGXxB+9H78hJCToZofVTbVtL5vaJjonB1USFVSiXCVSUuWzFeMi/xfn9QnRgb/FQ2KDbE1dFdFjx2QgzfVgy0gekitkS5d2Ue9nVI/z2jLRRAoD/lggKUjcXuqiaB4d8RKERYUDN/ED5zNhIjy8ChXRcezLJAUFCikIZGJCNCjLZnPOY2Tno5maxAG3ZyZX8JkhKfhrxiTWiCwlJhljnYnc3AZne8puDrQkajj9q+wf+36/+7YoeaJOAjlCUtCCQiKPTGxkE1rpXI+nMCFxIN/hviJ/ExNHdhI/aEEhsUeVthtQixNZcjcnzNlA4oKO1dEPoSCLqtN95gixQ4FCPJPPwcaZjVLlU+KGLAxSdE6/80EQEgRuy5ypbapjdK4hS3dvP5/u0iwhFChEC1lyJ1UZ3XPplDdxUtUZ9Jzr8PZzc9AkhYJqSdR0+cXtuXb6wsh8UuJAXO4jaChQiBaqteTUNi/nMi3vJVzSBDrtkbgjEufAiYLEr6RtJvviMjmIy30EDQUKMcI+iJmmxvbzxS9bpvF6Ha/WoFxAgUSCwuukww1VxufU35wcECcUKMQIrzMD0yUh2T6Z052XUEmRX4vKHyVf+HFtDvTEBFm/z6Yv2icROrmIuLxKnFCgkLxgas6VWUTcHO1MogWcy0W5mj16wY/rB30PJPykRIHKGpnNpMR+PpGPl58+LiR+UKCQ0OA08bpFGsj8UfyeiQXhyJfNSyFXibdIPJD1Z7coHBNEz6yXhIf2pR/258KDAiXPcBYgx5l5UiU0ZOvV2TrRigSS7BpRhv2wcFFZTEydWmWIJg3OZ9okLYDsvCTeMJMsCR26syXVYKWTqE1lgTHNSBsmolx3Eg78FgKqrLEmzzv7c/RhJlkSaXTERaqcysIi266axYnM3Lmctek6A3s9n8ivhrNQYkfWH/xcKpU5tZscT3FSeFCgkEjhjAyQLcGIBIipP0s+BkS3jLh+1iEMEUokfIhEuRd/EbdrpBDlWzE53n4eEm+4xEMih9e1a6+5ToJ8oZu+IFRr/4SoEAl4P5K1peAyDgFyuMQzf/58nHvuuSgtLUVpaSlqamqwfPny9P7Dhw+joaEBAwcORP/+/TFx4kS0tbVlnKO1tRXjx49Hv379UFFRgdmzZ+PYsWMm1SARJN+zHdWMyyQhVNAOeqbRDtmcgxBnFJiffd4k9N/knLSkxBcjgXL66adj3rx5aGlpwVtvvYXvfve7+P73v4/t27cDAGbOnImXX34ZS5Yswbp167B3715MmDAhffzx48cxfvx4HD16FOvXr8ezzz6LhQsXYu7cuf7eFQkduXpJxjldti6yiKdCaweSHU4xnqvlHfv5/Tg3+3l8yXqJZ8CAAXj44Ydx3XXX4bTTTsOiRYtw3XXXAQB27tyJESNGoLm5GePGjcPy5ctx9dVXY+/evaisrAQALFiwAHPmzMFnn32G4uJirWtyiYfIZnluSxoqK4PMnyUIs3M2fiiM4iHZkkurhDOVACks8hLFc/z4cSxevBiff/45ampq0NLSgq6uLtTW1qbLDB8+HEOGDEFzczMAoLm5GSNHjkyLEwCoq6tDZ2dn2goj4siRI+js7Mz4kMJBNdvSESeqF7bbABnUIOqHQ6uXY2kuJ05y4ahNcUJ0MBYoW7duRf/+/VFSUoLbb78dS5cuRXV1NZLJJIqLi1FeXp5RvrKyEslkEgCQTCYzxElqf2qfjKamJpSVlaU/gwcPNq02iTBOM64s1Fg26Nn328s4I4LsZYPCzfrhJiDcEtx5uSYpbLLNJKt73iAyNpNwYyxQzj77bGzevBkbNmzAtGnTMHnyZOzYsSMXdUvT2NiIjo6O9GfPnj05vR4JJ3aB4RQdOseKLCupc2WbgdYv3JamTAd+WftQlBAVMpFr4rQtOs7ej92smWF5JklwFJkeUFxcjG984xsAgNGjR2PTpk149NFH8YMf/ABHjx5Fe3t7hhWlra0NiUQCAJBIJLBx48aM86WifFJlRJSUlKCkpMS0qiRm2HMzqHDLWCnLjSLLB5EvdNKD6x4vypZrmnuCECdexYK9/5laK9lXC5esPUy7u7tx5MgRjB49Gr1798bq1avT+3bt2oXW1lbU1NQAAGpqarB161bs378/XWbVqlUoLS1FdXV1tlUhBYD9Ze184ar8NlL7VeIlaKuCaCnLtB7ONgjL0hWJJtn2G0bZkWwwEiiNjY147bXX8NFHH2Hr1q1obGzEq6++ivr6epSVlWHq1KmYNWsW1q5di5aWFkyZMgU1NTUYN24cAODyyy9HdXU1Jk2ahC1btmDlypW499570dDQQAsJOQEvlhLVMaLyTtESBrOyiYNsSliJUokzBJP4gVeRDIgteXa4fENUGIUZT506FatXr8a+fftQVlaGc889F3PmzMHf//3fA/gyUdtdd92FF154AUeOHEFdXR2eeOKJjOWbjz/+GNOmTcOrr76Kk046CZMnT8a8efNQVKS/2sQw48JEZCZOoXIoVUX3iMKUg8JrZIMsPJoQP1GJCbcwffvfpLAxCTNmqnsSOdwypqpSdGe7hBI2ZDPUoPK3kHhiYulw9j2dfsi+Wjjw14xJrJH5oIj2ZxNy62dZL5iEFIuch2UijRA72aagly2zipIpujm5s78SOxQoJPTIwmZl1gPdQc75YtedxeXK8dQ5iLudXxWmSedYooPK2uhEFQLv5qQuO4csoo4QgAKFRAC3iBTRTE10Dh3/lTA4xzr/n8Jk9smBnuggWgp08zURYRfVbpY90fno0E1EUKCQyCBL7mQfCEWhuql/ZULGuU+nHkHhJsLs/wL6iexI4aJyMPfrfLL9Mqsn+ywBPCRqIyRI3LKiOrerrC1hRVU3nWgkUdg0ISqcfcgkiZpsmcZkmVK1XEkKF1pQSKRwOr7K8n24DY7ZZMT0G13RJTODi0I5aTnJH1FvZ7fkhbK+JFqe8SJyCJHBMGMSKdyWN1Jl7GvhoqUdkT9KlAZUt1BqOhySbDFxnPXSD/3qo+zr0YJhxiS2mFgQZOIEEPtm5Hsm7KxP6l83R9hsXxzEP+LYvjp9TMeKmSqnOocfUJzEFwqUPBHHgSyXmLaXaA1bZmpW+WeYXtfL9yqqlzP6QYZpxBEHb2KCWx+0ixK354tjHskWCpQ8wReFGdmE/upaR0Thxqbfk5fv1S0XhPP/qlwRuiKLL4vcEPXnWhVyL/rbbUlUFGZMiFdiJVD4MMQTndBawN007RQgbi+XXPcne32d0Q/2Mtk6+3KNnohQiWE3HyfROZz72e9ItsRGoPBhCIagRKFzDdy+3YnMv0NnNugnKouNzsvA/reOCd6tLhT02RH19lM9P6pJgY6IsZ/LeQ1CdGEUDwk1IsuCrnAQDbwyp9l8iFtVOKcbosFeR4zIrkcxT5z9QGbFS6EroglRwSgeEnlM8oDIUCWBsr/kgxQn9jqJ/m8/XrRN5kOg4+hIChv7cyDyd3LitpxDiN/QgkJCjzOviU5Z5zYVQbywReZx0QxVti11DtF2+zVE20n2xK1tZZZFEaLw/bi0A8k9tKCQWKB6Mcv8SdysBiJH2VzO/mQzU9F1VUtSznKq7amPyIrCmS6R4WXplOKE5BIKFBJKZA54shezyFydwnkev2Z/OuZwke+Izjq+VyFhkkeFZEec2tJEqDvFvs7yECFe4BIPCRVuSzSqJQ/d85se4xWT5SZdh1ed/V6dioke+XauzicmIoPLO8QLXOIhsUVmQTEhaKdYWehwNuJFdO58OQAXGm6h3oUEQ4lJLqFAIaHCy1KHSehuvl4qXiJoZPt0B37mNskfFCdfEmdrEgkeChQSalRhs3HwtRDdk1vdVeHTUbnvKBNnIajbf0TLO3FtExIcFCgktJhmfhUdE1bcHAxVLwrRMRQm+SGovpXr6/rhe8I+SPyGAoVICdo7X2URsIfS6h4TVnT8VVRh0lG73yiTau98t3mur+cl3D4KEwESbRjFQ0KLKlTXud3kHEFjEuYsM6OH6X4KjXxFruTTQua2XOOWMJD9kehiEsVDgUJCiTN7rCrTpZtzqfN40XXygcm1ZOKML4L4YdovctEHTHKgiMQM+yXRhQKF+EpQA5BuThDT6JewDKaqPClBCikiJ24WLLccQyri0gYkvzAPCsmKbBKh+YnT18KOLJW783iZ70Y+nGlVPjy64iRuL8QoE7RQV/3EgVfsPjVuEwLnv27XpY8KyRYKFHICYXgZOp1gnS/qbOqoM8jnKiGcaNlJ5WsQhu+CZBIGwa6zXRcdsS7rqzqihhCvUKAUMGGe4chmdU6fFBk64bvO/zuPcYY4mwy4MtGhWrrhgB5uUn3E/kKX9cNcWDt0HKp1z2OKyFrJZUeSa7ISKPPmzUOPHj0wY8aM9LbDhw+joaEBAwcORP/+/TFx4kS0tbVlHNfa2orx48ejX79+qKiowOzZs3Hs2LFsqkI8EPbBRTYAqoRECufSjv2F4bTKOK/lx0CsqrdbORJ+nH0ll9+troBV5dVJYRf4JkLFbtGkoCb5wrNA2bRpE5588kmce+65GdtnzpyJl19+GUuWLMG6deuwd+9eTJgwIb3/+PHjGD9+PI4ePYr169fj2WefxcKFCzF37lzvd0FihXOGqpMjRDdni3NgluVTEV1Hp65uLwbZ3yQaOAWJzJois/KpBI3zfKJ9KnSEuwiVL5epAzohfuJJoBw6dAj19fV4+umnccopp6S3d3R04D//8z/xy1/+Et/97ncxevRoPPPMM1i/fj3efPNNAMD//u//YseOHXjuuecwatQoXHnllfj5z3+Oxx9/HEePHvXnrkikkTmPqso4xYrJMaJruDng6mBfDnC+sChOoouob5g6NrsJAq8ixa28zPIj65u6S5WE5AJPAqWhoQHjx49HbW1txvaWlhZ0dXVlbB8+fDiGDBmC5uZmAEBzczNGjhyJysrKdJm6ujp0dnZi+/btwusdOXIEnZ2dGR8Sb7z6e8j+L3qh2F8kbi8G0fVEVhA38zlnnvHCvoSo8/KX4eyrov7v7KfOfmZi7ZA9JzJMQ5AJ8QNjgbJ48WK8/fbbaGpqOmFfMplEcXExysvLM7ZXVlYimUymy9jFSWp/ap+IpqYmlJWVpT+DBw82rTaJEKbr4ypriux8svObCCOn1UYW0ixas+dgH31E4sCrL4qO1VAHlUgRiRqd89DnhASFkUDZs2cP7rzzTjz//PPo06dPrup0Ao2Njejo6Eh/9uzZk7drE39RDY46pnHZMSJfAJGFJNswXpHVRFUHHcsMxUo0US3rePExMu0Hzki3bP2pRPWg5YQEiZFAaWlpwf79+3H++eejqKgIRUVFWLduHR577DEUFRWhsrISR48eRXt7e8ZxbW1tSCQSAIBEInFCVE/q71QZJyUlJSgtLc34kPjg9jJXzQq95IZwvlhMwjNNIilEYcwi6w5nptFHR/i6Lau4+Xjo+IO4WTucfVPn2WL/JEFhJFAuu+wybN26FZs3b05/xowZg/r6+vT/e/fujdWrV6eP2bVrF1pbW1FTUwMAqKmpwdatW7F///50mVWrVqG0tBTV1dU+3RYJK7K19RRu6+Vu5xIdqxvdoHqBZCNOdLeReOMlvNcN57KNjt+LKOrNTWDRikKCIOvf4vnOd76DUaNG4ZFHHgEATJs2Da+88goWLlyI0tJS3HHHHQCA9evXA/gyzHjUqFGoqqrCQw89hGQyiUmTJuGf/umf8OCDD2pdk7/FE19MQiNlpmindcIuLuxldESCmxOiiWlfVB8Sb3L1XTv7scj6qFqecYtgYx8luSLQ3+L51a9+hauvvhoTJ07EpZdeikQigRdffDG9v1evXli2bBl69eqFmpoa3Hzzzbjllltw//33e76myaybhBddq4JqtuecpcoGbxMLhirk2BlZoXLMtV+X/ZN4QSWYvTqCp/qwSmSzv5Ig4K8Zk9Dj5pOi68PiJcJCdm632anKmqNzXUKciKwjzn91YV8kQVEwv2Z87Vkjg64CyQHOdXXVbM5tlug2cOvuc1vqER0nEye61yfxI5vvWxWubnreVJ9k6DsJM7GxoHDNNNqYzv5kx4r2ebWgOPuUjvVEVV8TkUPiTTbjlcr/RKdvivy33OrI8ZX4RcFYUOy4hdVxdhBuTPOUyMSG0+ohG7B1rCCytXrn/53nFIkk+0uFAz3Jpg84+6EoikeFmyO3qI7ssyQIYiNQSDxwc/RTCRnZEopon84sU7euqReGfeB3C6cmhY2fEyYv/cpphaE/CgkjsRAoOg+7ysxvei7iL6KIF13LiWzmqPJT8XMQ1s19woG/sPGrDzr7tSxaTIZMOLN/kjASGx8UN7iGGl68DrC6YZSysvnqDzKfAUK8oopc8xphRkg+MPFBibRA+Q6+j9X7xL+AnCv4MPuPzBlVhK5AcXMYzOV3SOdCki+8WnzZJ0lQFIyT7NL3tgZdhawxNdHGEV3H2BQ6y3T5HoBl6/gix11C/CAbx2uGvJMoEGmBEgScefiPMwpBd6BUDcxBDLZOczsHfJIrVEs8MkxC6gkJAxQoIYADw5e4ORK6OZwGmXhKlZmTyz3EC6b9182Sosp9Yprbh5B8EGuBUqgPXJTu22mm9iow/JpJZoPbUhXFSbTJ93NlEuqe+r9u+LwsAihKYweJP5F2knWL4gn7jDXs9cs1upEHdkzaK8jIHWc9Cvl7jiNu2VyDqIsK0fPFPkmCoGCcZHWgE2q40U1yptruJGzfN18E8cOZzRUIrt+pMr466ybKckxIWIm1QBEtG4SJXKz7Rm3A8SuawOmYKjtHkII1at8N+Qq/rHt+9D+TpIQ6SzjslySsxFqghP3Bc8vV4eV8UUHXIVb3XCbtGFbBSqKFbAlR1w/EK8709DrnFVlZ3CKBojSekHgSax8UEhyqtXjn+r0dP5Kr5foFQQoPv3xL/D6PzvPjJppkmZf5nJBcQB8UkhNMo2p0EqrZy4uuYfJbNiZRDIQEgV/9zyT/j85zSAdaEkZiI1CYrTM3OAcvVds6k62ZzNxUPiO69eQP85Fc4UdGYD/9n9xy7pgiehY5jpKgKQq6An7hDPPLlYmykE2fbvctEghuIsQtqsD+YnC2vWzNPNslIkJS6FgfdK17fvY/XQuKzpKp6ZIQIfkitj4oshlGNhSyOMkGlc+JE1kZWb4JUS4VmYgJ8rtj34kHpt9jLschXedwL8kP2VdJrqAPCtzTPns9JzFH1G6i78c0EsckpJLfHfED58ve7+SCJnXw2z/Lfn7mjyJhIDZLPCpyMYsh2eHmzGcXIDpRBl5DPvMF+148MO1PbonS8oFXEcU+S4ImthYUooeO45/OAOc245L5pYj+bzdfq/I3pK4pGvzDNAMMU12IOSbRaDL8suhm049U17c/S1yOJGGhICwofNjUOH1EvEYG6JRzWkZE/3eLFFI5CDrN3mERBuyD0UYkmPP9Ihf5YKX+L0L2fLmVJSQs0IISAoKcYacG3WwHWpMkaqKBVRYm7CU/gyzqJyjCUAfiHdnz4cVZNtt6yK4tqqPIOmqSP4WQoKFAKRB0zdRePP5N8JrAzeTcXpwIcwkH/8JAlTPIz+Ud53VkTq0y3y2d+nBZkoQBCpQQkIuIIy91AE7060ht8wuVgMh2QNdZIiLEb2T9NpfROzpLTSIxIloikl3Hfg5CgiC2eVCIOc78Cn6fG5APfs68DqJ8J25Ofk6CFn0pwrLUROKJyHIis5QEERZNiJ2c5UH52c9+hh49emR8hg8fnt5/+PBhNDQ0YODAgejfvz8mTpyItra2jHO0trZi/Pjx6NevHyoqKjB79mwcO3bMpBrEZ+wWk1zNmFKzPpNIIS8WlTAOrmGsE4kvKuun3edM5Y/CJR4SBoyjeL75zW/i97///VcnKPrqFDNnzsT//M//YMmSJSgrK8P06dMxYcIEvPHGGwCA48ePY/z48UgkEli/fj327duHW265Bb1798aDDz7ow+0QL+TrBSqzjKjqo2M1EYUnc3AlcUQWZaeK5DGFgpqEBaMlnp/97Gd46aWXsHnz5hP2dXR04LTTTsOiRYtw3XXXAQB27tyJESNGoLm5GePGjcPy5ctx9dVXY+/evaisrAQALFiwAHPmzMFnn32G4uJirXpwicdfglyC8CuBFZdRSKHiJsZNIneCDKMmhUFOU92///77qKqqwte//nXU19ejtbUVANDS0oKuri7U1tamyw4fPhxDhgxBc3MzAKC5uRkjR45MixMAqKurQ2dnJ7Zv3y695pEjR9DZ2ZnxIf6Rj0FIJ4mbKAGbrqk5rANpIVtygrr3Qmlz3edC9gy5OeeH8ZkqlO+WfImRQBk7diwWLlyIFStWYP78+di9eze+/e1v4+DBg0gmkyguLkZ5eXnGMZWVlUgmkwCAZDKZIU5S+1P7ZDQ1NaGsrCz9GTx4sEm1SQjQiRiQbTPJfRImwlinfBL0Ulvc29/EL0uW1C1qVpIo1ZVkj5FAufLKK3H99dfj3HPPRV1dHV555RW0t7fjt7/9ba7qBwBobGxER0dH+rNnz56cXo8EQzYhmmEcuMJYp3wgis4yOSYbYRHmbMJe8bqEkzpW5WwuC10mJAxk5cBRXl6Os846Cx988AESiQSOHj2K9vb2jDJtbW1IJBIAgEQicUJUT+rvVBkRJSUlKC0tzfiQ6GD6grD7pXDQjB5eLCd+iQlRWG3U+5BOUrVsji8Uoi5UC5GsBMqhQ4fwf//3fxg0aBBGjx6N3r17Y/Xq1en9u3btQmtrK2pqagAANTU12Lp1K/bv358us2rVKpSWlqK6ujqbqpAQEwdriFfidC9uiGbqJlFVQWUuzlddskHHh0t1bNjuhxAdjATK3XffjXXr1uGjjz7C+vXrce2116JXr1646aabUFZWhqlTp2LWrFlYu3YtWlpaMGXKFNTU1GDcuHEAgMsvvxzV1dWYNGkStmzZgpUrV+Lee+9FQ0MDSkpKcnKDhARFIb4UZGnYU/tUx6XK+02cQ23dUtxnk5E5bkTlOyVfYSRQPvnkE9x00004++yzccMNN2DgwIF48803cdpppwEAfvWrX+Hqq6/GxIkTcemllyKRSODFF19MH9+rVy8sW7YMvXr1Qk1NDW6++WbccsstuP/++/29K0ICptDS7od1WS4u7W/iZC7KN6T7ncRhSYzEB6a6JyRHcLCPHrn8uQe/MVk+0xUyhOSanOZBIYToEdcBP6qp0E3qG/bvzlScqJKyUZyQsEKBQkgOiOIL3A3nMk6YXmpRTuZngujnHXSOUf3asfP/qb/j1n9J9KBAISQHxOFl6ET2S9RhwI/2Dts9ieCvE5NCggKFkBwRxxdDLqNtwkCY78tLFlid3+ERWcPCZiEjhQkFCiE5IAqzcS+YvrRMlgqyjXzKNgNt2HEms5P9TISOlcXke4xC25B4QoFCCJHiR7i0yW/GBOGwGZUXsEjsiXLOONsw2yy9tKSQoGCYMSGk4IlKJIszekf2fzsy4ReVeybxgmHGJFZEMelZlOqqIg5p4nWIyotaR5yofiFc9cOBhIQNChQSejiQBkec08SniFJIrTO5mn27CNmvF7sdR0gYoEAhkSEqL744DPpxuAcTotK37Ms1qjqb+P0QElYoUEjguL0MRfsL7QWab/jiCi8mv9DM54REGQoUEjiyLJduUQthJOz1IycSNTEmcm5VhRyzT5KoQoFCIoGOn0AYfAlMUpCT/BN0//ALmbOrykHW9LyEBA0FCgkVzt97MYXCgKiISv8QRa45BbhKTFBokDhAgUICxW2QlZmys50pEhJmRP1c9n9ReDGfBRIHKFBIoMiyXMqEC2eGpBDQtZQAYqdZZo0lcYAChYQC+4zRHkYpy4ApIgzihdk5id+4WVF0+lwYng1CTGGqexI4sgyYqkGVIoBkQ5R/lVknKRtT25OwwlT3JBCyNSs7HQNljrI6PyFPSKFhf368+Gjx+SFhoyjoChAiSt/tZbAMw+yQs1SSb5xLoex/JC7QgkKywg/nPOdaut1yopOQihATomApMK2jH88Fny0SNihQSFY4nfayGeREa+hReJnY4SAfHcL8XZlEt/n1jETtWSPxhwKFhJIwvzxItIlK3zJJae8HUWkXUjhQoJBQIkvlTUi2ZOPnlE90+33Y74MQr1CgkNDgFCUmLxIO0qRQMQnPJyRKUKCQQLH/vkg2DrG0shATsvWXChN+CBKKGhJGGGZMAkX2myJc4iG5Iu79iT+ySeICLSgkNKjS2wOc5ZHCgP2ckC+hQCGhQ+cXXAmJK0H0c4oiEkaMBcqnn36Km2++GQMHDkTfvn0xcuRIvPXWW+n9lmVh7ty5GDRoEPr27Yva2lq8//77Gec4cOAA6uvrUVpaivLyckydOhWHDh3K/m5IrKAgIcQdP/IF8VkjYcRIoPzlL3/BxRdfjN69e2P58uXYsWMH/v3f/x2nnHJKusxDDz2Exx57DAsWLMCGDRtw0kknoa6uDocPH06Xqa+vx/bt27Fq1SosW7YMr732Gm677Tb/7ooQQgoIRvKQOGL0a8b33HMP3njjDfzhD38Q7rcsC1VVVbjrrrtw9913AwA6OjpQWVmJhQsX4sYbb8S7776L6upqbNq0CWPGjAEArFixAldddRU++eQTVFVVudaDv2ZMCHEjrr+L5BQfcbxHEl9y9mvGv/vd7zBmzBhcf/31qKiowHnnnYenn346vX/37t1IJpOora1NbysrK8PYsWPR3NwMAGhubkZ5eXlanABAbW0tevbsiQ0bNgive+TIEXR2dmZ8CCGk0KBlhBQSRgLlww8/xPz58zFs2DCsXLkS06ZNw49+9CM8++yzAIBkMgkAqKyszDiusrIyvS+ZTKKioiJjf1FREQYMGJAu46SpqQllZWXpz+DBg02qTQgpMArhRS4K0SckThgJlO7ubpx//vl48MEHcd555+G2227DrbfeigULFuSqfgCAxsZGdHR0pD979uzJ6fUIscPBP5rEeenDr1xB7NskzBgJlEGDBqG6ujpj24gRI9Da2goASCQSAIC2traMMm1tbel9iUQC+/fvz9h/7NgxHDhwIF3GSUlJCUpLSzM+hOSLOL/o4kghvXSz7Zvs2yTMGAmUiy++GLt27crY9t577+GMM84AAAwdOhSJRAKrV69O7+/s7MSGDRtQU1MDAKipqUF7eztaWlrSZdasWYPu7m6MHTvW840QQggQrzT2IlL3Fud7JAQwTHU/c+ZMXHTRRXjwwQdxww03YOPGjXjqqafw1FNPAQB69OiBGTNm4Be/+AWGDRuGoUOH4r777kNVVRWuueYaAF9aXK644or00lBXVxemT5+OG2+8USuChxBCCpFCsgwRAhgKlAsuuABLly5FY2Mj7r//fgwdOhSPPPII6uvr02V+/OMf4/PPP8dtt92G9vZ2XHLJJVixYgX69OmTLvP8889j+vTpuOyyy9CzZ09MnDgRjz32mH93RQghMYRWE1JIGOVBCQvMg0IIKTTimteFFBY5y4NCSNig2ZsQQuKJ0RJPWEgZfToPdQdcExI0S3a9jc6DQdeCkNzDvk7iQOq9rbN4E0mB8uc//xkAcMb5HwVbEUIIIYQYc/DgQZSVlSnLRFKgDBgwAADQ2trqeoPkRDo7OzF48GDs2bOHOWUMYdt5h22XHWw/77DtvON321mWhYMHD2pF7UZSoPTs+aXrTFlZGTtbFjDpnXfYdt5h22UH2887bDvv+Nl2uoYFOskSQgghJHRQoBBCCCEkdERSoJSUlOCnP/0pSkpKgq5KJGH7eYdt5x22XXaw/bzDtvNOkG0XyURthBBCCIk3kbSgEEIIISTeUKAQQgghJHRQoBBCCCEkdFCgEEIIISR0UKAQQgghJHREUqA8/vjjOPPMM9GnTx+MHTsWGzduDLpKgdPU1IQLLrgAJ598MioqKnDNNddg165dGWUOHz6MhoYGDBw4EP3798fEiRPR1taWUaa1tRXjx49Hv379UFFRgdmzZ+PYsWP5vJXAmTdvHnr06IEZM2akt7Ht5Hz66ae4+eabMXDgQPTt2xcjR47EW2+9ld5vWRbmzp2LQYMGoW/fvqitrcX777+fcY4DBw6gvr4epaWlKC8vx9SpU3Ho0KF830reOX78OO677z4MHToUffv2xd/8zd/g5z//ecYPqbH9vuS1117DP/zDP6Cqqgo9evTASy+9lLHfr3b64x//iG9/+9vo06cPBg8ejIceeijXt5ZzVG3X1dWFOXPmYOTIkTjppJNQVVWFW265BXv37s04RyBtZ0WMxYsXW8XFxdZ//dd/Wdu3b7duvfVWq7y83Gprawu6aoFSV1dnPfPMM9a2bduszZs3W1dddZU1ZMgQ69ChQ+kyt99+uzV48GBr9erV1ltvvWWNGzfOuuiii9L7jx07Zp1zzjlWbW2t9c4771ivvPKKdeqpp1qNjY1B3FIgbNy40TrzzDOtc88917rzzjvT29l2Yg4cOGCdccYZ1g9/+ENrw4YN1ocffmitXLnS+uCDD9Jl5s2bZ5WVlVkvvfSStWXLFut73/ueNXToUOuLL75Il7niiiusb33rW9abb75p/eEPf7C+8Y1vWDfddFMQt5RXHnjgAWvgwIHWsmXLrN27d1tLliyx+vfvbz366KPpMmy/L3nllVesn/zkJ9aLL75oAbCWLl2asd+Pduro6LAqKyut+vp6a9u2bdYLL7xg9e3b13ryySfzdZs5QdV27e3tVm1trfWb3/zG2rlzp9Xc3GxdeOGF1ujRozPOEUTbRU6gXHjhhVZDQ0P67+PHj1tVVVVWU1NTgLUKH/v377cAWOvWrbMs68tO2Lt3b2vJkiXpMu+++64FwGpubrYs68tO3LNnTyuZTKbLzJ8/3yotLbWOHDmS3xsIgIMHD1rDhg2zVq1aZf3t3/5tWqCw7eTMmTPHuuSSS6T7u7u7rUQiYT388MPpbe3t7VZJSYn1wgsvWJZlWTt27LAAWJs2bUqXWb58udWjRw/r008/zV3lQ8D48eOtf/zHf8zYNmHCBKu+vt6yLLafDOdL1q92euKJJ6xTTjkl45mdM2eOdfbZZ+f4jvKHSNw52bhxowXA+vjjjy3LCq7tIrXEc/ToUbS0tKC2tja9rWfPnqitrUVzc3OANQsfHR0dAL765eeWlhZ0dXVltN3w4cMxZMiQdNs1Nzdj5MiRqKysTJepq6tDZ2cntm/fnsfaB0NDQwPGjx+f0UYA207F7373O4wZMwbXX389KioqcN555+Hpp59O79+9ezeSyWRG25WVlWHs2LEZbVdeXo4xY8aky9TW1qJnz57YsGFD/m4mAC666CKsXr0a7733HgBgy5YteP3113HllVcCYPvp4lc7NTc349JLL0VxcXG6TF1dHXbt2oW//OUvebqb4Ono6ECPHj1QXl4OILi2i9SvGf/pT3/C8ePHM14CAFBZWYmdO3cGVKvw0d3djRkzZuDiiy/GOeecAwBIJpMoLi5Od7gUlZWVSCaT6TKitk3tizOLFy/G22+/jU2bNp2wj20n58MPP8T8+fMxa9Ys/Mu//As2bdqEH/3oRyguLsbkyZPT9y5qG3vbVVRUZOwvKirCgAEDYt12AHDPPfegs7MTw4cPR69evXD8+HE88MADqK+vBwC2nyZ+tVMymcTQoUNPOEdq3ymnnJKT+oeJw4cPY86cObjpppvSv14cVNtFSqAQPRoaGrBt2za8/vrrQVclEuzZswd33nknVq1ahT59+gRdnUjR3d2NMWPG4MEHHwQAnHfeedi2bRsWLFiAyZMnB1y78PPb3/4Wzz//PBYtWoRvfvOb2Lx5M2bMmIGqqiq2H8k7XV1duOGGG2BZFubPnx90daIVxXPqqaeiV69eJ0RPtLW1IZFIBFSrcDF9+nQsW7YMa9euxemnn57enkgkcPToUbS3t2eUt7ddIpEQtm1qX1xpaWnB/v37cf7556OoqAhFRUVYt24dHnvsMRQVFaGyspJtJ2HQoEGorq7O2DZixAi0trYC+OreVc9sIpHA/v37M/YfO3YMBw4ciHXbAcDs2bNxzz334MYbb8TIkSMxadIkzJw5E01NTQDYfrr41U6F+hwDX4mTjz/+GKtWrUpbT4Dg2i5SAqW4uBijR4/G6tWr09u6u7uxevVq1NTUBFiz4LEsC9OnT8fSpUuxZs2aE0xto0ePRu/evTPabteuXWhtbU23XU1NDbZu3ZrREVMd1fkSihOXXXYZtm7dis2bN6c/Y8aMQX19ffr/bDsxF1988Qnh7O+99x7OOOMMAMDQoUORSCQy2q6zsxMbNmzIaLv29na0tLSky6xZswbd3d0YO3ZsHu4iOP7617+iZ8/MYbhXr17o7u4GwPbTxa92qqmpwWuvvYaurq50mVWrVuHss8+O9fJOSpy8//77+P3vf4+BAwdm7A+s7Ty71wbE4sWLrZKSEmvhwoXWjh07rNtuu80qLy/PiJ4oRKZNm2aVlZVZr776qrVv3770569//Wu6zO23324NGTLEWrNmjfXWW29ZNTU1Vk1NTXp/KlT28ssvtzZv3mytWLHCOu2002IfKivCHsVjWWw7GRs3brSKioqsBx54wHr//fet559/3urXr5/13HPPpcvMmzfPKi8vt/77v//b+uMf/2h9//vfF4Z/nnfeedaGDRus119/3Ro2bFjswmRFTJ482fra176WDjN+8cUXrVNPPdX68Y9/nC7D9vuSgwcPWu+88471zjvvWACsX/7yl9Y777yTjjTxo53a29utyspKa9KkSda2bdusxYsXW/369Yt8mLGq7Y4ePWp973vfs04//XRr8+bNGe8Pe0ROEG0XOYFiWZb161//2hoyZIhVXFxsXXjhhdabb74ZdJUCB4Dw88wzz6TLfPHFF9Y///M/W6eccorVr18/69prr7X27duXcZ6PPvrIuvLKK62+fftap556qnXXXXdZXV1deb6b4HEKFLadnJdfftk655xzrJKSEmv48OHWU089lbG/u7vbuu+++6zKykqrpKTEuuyyy6xdu3ZllPnzn/9s3XTTTVb//v2t0tJSa8qUKdbBgwfzeRuB0NnZad15553WkCFDrD59+lhf//rXrZ/85CcZLwa235esXbtWOMZNnjzZsiz/2mnLli3WJZdcYpWUlFhf+9rXrHnz5uXrFnOGqu12794tfX+sXbs2fY4g2q6HZdlSFhJCCCGEhIBI+aAQQgghpDCgQCGEEEJI6KBAIYQQQkjooEAhhBBCSOigQCGEEEJI6KBAIYQQQkjooEAhhBBCSOigQCGEEEJI6KBAIYQQQkjooEAhhBBCSOigQCGEEEJI6Ph/9w+DFC7EdNMAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bjBDj97y9k18",
        "3jKEwPDK-Edp",
        "Kja3q7PyhB4H",
        "s2Fd1wKnhUqf",
        "V5OVRWSXYUo4",
        "HKc6MK8Hy_YJ"
      ],
      "name": "SAR_and_Camera_PointCloud.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}