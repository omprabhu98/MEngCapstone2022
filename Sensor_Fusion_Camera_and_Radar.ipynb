{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBDj97y9k18"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwz1UzMu9mF-",
        "outputId": "81b9cb4b-b547-4b9c-cb7f-0c9fbd56bea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# classic libraries\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from math import *\n",
        "\n",
        "\n",
        "# I/O libraries\n",
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "import subprocess as subp\n",
        "\n",
        "#data analysis libraries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import pandas as pd\n",
        "\n",
        "#image processing libraries\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow.compat.v1 as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "#others libraries\n",
        "from tqdm import tqdm\n",
        "import IPython\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "# Comment this out if you want to see Deprecation warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "#test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwkf2tnYQKAT",
        "outputId": "eadd0cde-a979-4397-af8e-11ff127fe8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 13.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading our github"
      ],
      "metadata": {
        "id": "aWjfYK2VUl8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/Mmengyw/Capstone-Updated.git\n",
        "!ls\n",
        "os.chdir(\"Capstone-Updated/Videos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLJkxmhmUk3u",
        "outputId": "dcfbe516-9144-4058-e96d-0cd0ff43809f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Capstone-Updated'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 238 (delta 8), reused 13 (delta 6), pack-reused 222\u001b[K\n",
            "Receiving objects: 100% (238/238), 217.27 MiB | 22.21 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n",
            "Checking out files: 100% (37/37), done.\n",
            "Capstone-Updated  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_9rEI7rvhwW"
      },
      "source": [
        "# Functions for Image Segmentation ???"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class to load deeplab model and run inference\n",
        "\n",
        "\n",
        "Definition of the class DeepLabModel\n",
        "\n",
        "\n",
        "1.   function run:\n",
        "\n",
        "  *   Args:\n",
        "            - image: A PIL.Image object, raw input image.\n",
        "            - INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
        "            - OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
        "  *   Returns:\n",
        "            - resized_image: RGB image resized from original input image.\n",
        "            - seg_map: Segmentation map of `resized_image`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gGDDCulgbAQi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Xfj_GZ-FWL"
      },
      "outputs": [],
      "source": [
        "class DeepLabModel(object):\n",
        "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, tarball_path):\n",
        "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "\n",
        "        # Extract frozen graph from tar archive.\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
        "        \"\"\"Runs inference on a single image.\n",
        "\n",
        "        Args:\n",
        "            image: A PIL.Image object, raw input image.\n",
        "            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
        "            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
        "\n",
        "        Returns:\n",
        "            resized_image: RGB image resized from original input image.\n",
        "            seg_map: Segmentation map of `resized_image`.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        target_size = (2049,1025)  # size of Cityscapes images\n",
        "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "        batch_seg_map = self.sess.run(\n",
        "            OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
        "        if len(seg_map.shape) == 2:\n",
        "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
        "        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)\n",
        "        return seg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIUhvfV-MiI"
      },
      "outputs": [],
      "source": [
        "def create_label_colormap():\n",
        "    \"\"\"Creates a label colormap used in Cityscapes segmentation benchmark.\n",
        "\n",
        "    Returns:\n",
        "        A Colormap for visualizing segmentation results.\n",
        "    \"\"\"\n",
        "    colormap = np.array([\n",
        "        [128,  64, 128],\n",
        "        [244,  35, 232],\n",
        "        [ 70,  70,  70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170,  30],\n",
        "        [220, 220,   0],\n",
        "        [107, 142,  35],\n",
        "        [152, 251, 152],\n",
        "        [ 70, 130, 180],\n",
        "        [220,  20,  60],\n",
        "        [255,   0,   0],\n",
        "        [  0,   0, 142],\n",
        "        [  0,   0,  70],\n",
        "        [  0,  60, 100],\n",
        "        [  0,  80, 100],\n",
        "        [  0,   0, 230],\n",
        "        [119,  11,  32],\n",
        "        [  0,   0,   0]], dtype=np.uint8)\n",
        "    return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "    Args:\n",
        "        label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "    Returns:\n",
        "        result: A 2D array with floating type. The element of the array\n",
        "            is the color indexed by the corresponding element in the input label\n",
        "            to the PASCAL color map.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If label is not of rank 2 or its value is larger than color\n",
        "            map maximum entry.\n",
        "    \"\"\"\n",
        "    if label.ndim != 2:\n",
        "        raise ValueError('Expect 2-D input label')\n",
        "\n",
        "    colormap = create_label_colormap()\n",
        "\n",
        "    if np.max(label) >= len(colormap):\n",
        "        raise ValueError('label value too large.')\n",
        "\n",
        "    return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input image')\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "    plt.imshow(seg_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation map')\n",
        "\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(seg_image, alpha=0.7)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation overlay')\n",
        "\n",
        "    unique_labels = np.unique(seg_map)\n",
        "    ax = plt.subplot(grid_spec[3])\n",
        "    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "    ax.yaxis.tick_right()\n",
        "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "    plt.xticks([], [])\n",
        "    ax.tick_params(width=0.0)\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle', 'void'])\n",
        "\n",
        "COLOR_MAP = np.array([\n",
        "    [128,  64, 128],\n",
        "    [244,  35, 232],\n",
        "    [ 70,  70,  70],\n",
        "    [102, 102, 156],\n",
        "    [190, 153, 153],\n",
        "    [153, 153, 153],\n",
        "    [250, 170,  30],\n",
        "    [220, 220,   0],\n",
        "    [107, 142,  35],\n",
        "    [152, 251, 152],\n",
        "    [ 70, 130, 180],\n",
        "    [220,  20,  60],\n",
        "    [255,   0,   0],\n",
        "    [  0,   0, 142],\n",
        "    [  0,   0,  70],\n",
        "    [  0,  60, 100],\n",
        "    [  0,  80, 100],\n",
        "    [  0,   0, 230],\n",
        "    [119,  11,  32],\n",
        "    [  0,   0,   0]], dtype=np.uint8)\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKiAGG_-QYw",
        "outputId": "3caf15e0-afb8-4d17-f135-5f81a39e62eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading model, this might take a while...\n",
            "download completed! loading DeepLab model...\n",
            "model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
        "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_cityscapes_trainfine':\n",
        "        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
        "    'xception65_cityscapes_trainfine':\n",
        "        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnUbIVY96HU"
      },
      "source": [
        "# Initialize Midas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "197cda64e0e04cdc85e8befa58d9370f",
            "bd830649fc8042cdb7b3adf0162cd8bc",
            "f82bc16105b94f389c2fc136813415fb",
            "64b06064c97c4a9ab9b50d932fb55ae7",
            "3fe4a1d3b676440a9a2bb13168334f5a",
            "25ba942f3aa04b658652a452e4b7fa27",
            "606e0c079a624727bc5a835edaf5a09d",
            "17fa647c640c40ab885909ed7bb3b6ce",
            "12ffc61f5f43434d8bf16f06e1a09d21",
            "0cd3d9150c35424a976ff9ce062f054f",
            "1a2adeafa2a24bb090e163539214b04f"
          ]
        },
        "id": "uW1E_emiQatf",
        "outputId": "058a5b1d-4e2b-462a-9c41-b010ac79b6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/intel-isl/MiDaS/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\" to /root/.cache/torch/hub/checkpoints/dpt_large-midas-2f21e586.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/1.28G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "197cda64e0e04cdc85e8befa58d9370f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/\n",
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJyJpTzQtDR",
        "outputId": "643a2321-51cd-4f83-80e8-fcea26c279ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DPTDepthModel(\n",
              "  (pretrained): Module(\n",
              "    (model): VisionTransformer(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "        (norm): Identity()\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (blocks): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (12): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (13): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (14): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (15): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (16): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (17): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (18): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (19): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (20): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (21): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (22): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (23): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (pre_logits): Identity()\n",
              "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "    )\n",
              "    (act_postprocess1): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
              "    )\n",
              "    (act_postprocess2): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (act_postprocess3): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (act_postprocess4): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (scratch): Module(\n",
              "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (refinenet1): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet2): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet3): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet4): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Interpolate()\n",
              "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Identity()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Da96O0Q06Z",
        "outputId": "5efc6614-ebe9-4e89-f80c-f46b0e3c71db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0kU6NJ1Ye5",
        "outputId": "8629fb66-7e5d-417b-a09a-8bc2d9ca9ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosted Midas\n"
      ],
      "metadata": {
        "id": "uCBzhadMUZGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/\n",
        "# Clone git repo\n",
        "!git clone https://github.com/compphoto/BoostingMonocularDepth.git\n",
        "\n",
        "!wget https://sfu.ca/~yagiz/CVPR21/latest_net_G.pth\n",
        "#!gdown https://drive.google.com/u/0/uc?id=1cU2y-kMbt0Sf00Ns4CN2oO9qPJ8BensP&export=download\n",
        "\n",
        "# Downloading merge model weights\n",
        "!mkdir -p /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "!mv latest_net_G.pth /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "\n",
        "# Downloading Midas weights\n",
        "!wget https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21-f6b98070.pt\n",
        "!mv midas_v21-f6b98070.pt /content/BoostingMonocularDepth/midas/model.pt\n",
        "\n",
        "# # Downloading LeRes weights\n",
        "!wget https://cloudstor.aarnet.edu.au/plus/s/lTIJF4vrvHCAI31/download\n",
        "!mv download /content/BoostingMonocularDepth/res101.pth\n",
        "\n",
        "#creation of useful folders to run boosted midas\n",
        "!cd /content/\n",
        "!mkdir '/content/Depth_pred_folder/'\n",
        "!mkdir '/content/Depth_pred_folder/inputs/'\n",
        "!mkdir '/content/Depth_pred_folder/outputs/'\n",
        "\n",
        "!cd /content"
      ],
      "metadata": {
        "id": "_sSvm7zobTPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2977d7a9-6161-40e9-8f48-96f079b2aa60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'BoostingMonocularDepth' already exists and is not an empty directory.\n",
            "--2022-04-20 16:42:43--  https://sfu.ca/~yagiz/CVPR21/latest_net_G.pth\n",
            "Resolving sfu.ca (sfu.ca)... 142.58.228.150, 142.58.103.17, 142.58.103.137, ...\n",
            "Connecting to sfu.ca (sfu.ca)|142.58.228.150|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.sfu.ca/~yagiz/CVPR21/latest_net_G.pth [following]\n",
            "--2022-04-20 16:42:43--  https://www.sfu.ca/~yagiz/CVPR21/latest_net_G.pth\n",
            "Resolving www.sfu.ca (www.sfu.ca)... 142.58.228.150\n",
            "Connecting to www.sfu.ca (www.sfu.ca)|142.58.228.150|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 318268048 (304M)\n",
            "Saving to: ‘latest_net_G.pth’\n",
            "\n",
            "latest_net_G.pth    100%[===================>] 303.52M  14.1MB/s    in 21s     \n",
            "\n",
            "2022-04-20 16:43:05 (14.4 MB/s) - ‘latest_net_G.pth’ saved [318268048/318268048]\n",
            "\n",
            "--2022-04-20 16:43:05--  https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21-f6b98070.pt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/280183797/fbebca80-97de-11eb-8da9-8d9645bd6d6f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220420%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220420T164305Z&X-Amz-Expires=300&X-Amz-Signature=e6fe50a5f58d59c207890a60a84192e323eb3df07a0e07158c869110f09c1721&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=280183797&response-content-disposition=attachment%3B%20filename%3Dmidas_v21-f6b98070.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-04-20 16:43:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/280183797/fbebca80-97de-11eb-8da9-8d9645bd6d6f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220420%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220420T164305Z&X-Amz-Expires=300&X-Amz-Signature=e6fe50a5f58d59c207890a60a84192e323eb3df07a0e07158c869110f09c1721&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=280183797&response-content-disposition=attachment%3B%20filename%3Dmidas_v21-f6b98070.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 422509849 (403M) [application/octet-stream]\n",
            "Saving to: ‘midas_v21-f6b98070.pt.1’\n",
            "\n",
            "midas_v21-f6b98070. 100%[===================>] 402.94M  27.0MB/s    in 9.0s    \n",
            "\n",
            "2022-04-20 16:43:15 (44.7 MB/s) - ‘midas_v21-f6b98070.pt.1’ saved [422509849/422509849]\n",
            "\n",
            "mv: cannot move 'midas_v21-f6b98070.pt' to '/content/BoostingMonocularDepth/midas/model.pt': No such file or directory\n",
            "--2022-04-20 16:43:15--  https://cloudstor.aarnet.edu.au/plus/s/lTIJF4vrvHCAI31/download\n",
            "Resolving cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)... 202.158.207.20\n",
            "Connecting to cloudstor.aarnet.edu.au (cloudstor.aarnet.edu.au)|202.158.207.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Syntax error in Set-Cookie: 5230042dc1897=f8ia42de6fon9dssctsn58ac8q; path=/plus;; Secure at position 53.\n",
            "Syntax error in Set-Cookie: oc_sessionPassphrase=8oCbabb4dxD9s1wxrLML5TR9%2Bhs0iQPvFEMsXZ7CUjw3KLl1ODJqvIwJ%2BWh3bLl9FHSiw8m7g%2Fvf%2F4EcbwMZITCeiKk2lIft%2F19n8Z27dicAf%2Fcwkc%2FUM7gs590nVCkO; path=/plus;; Secure at position 176.\n",
            "Length: 530760553 (506M) [application/octet-stream]\n",
            "Saving to: ‘download’\n",
            "\n",
            "download            100%[===================>] 506.17M  12.3MB/s    in 43s     \n",
            "\n",
            "2022-04-20 16:44:01 (11.6 MB/s) - ‘download’ saved [530760553/530760553]\n",
            "\n",
            "mkdir: cannot create directory ‘/content/Depth_pred_folder/’: File exists\n",
            "mkdir: cannot create directory ‘/content/Depth_pred_folder/inputs/’: File exists\n",
            "mkdir: cannot create directory ‘/content/Depth_pred_folder/outputs/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gray scale results\n",
        "%cd /content/BoostingMonocularDepth/\n",
        "!python run.py --Final --data_dir /content/Depth_pred_folder/inputs/ --output_dir  /content/Depth_pred_folder/outputs/ --depthNet 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK3bfmMefDK7",
        "outputId": "0b8a1c24-5b22-4508-a8c3-780fc971c7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BoostingMonocularDepth\n",
            "python3: can't open file 'run.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth prediction on 1 frame"
      ],
      "metadata": {
        "id": "BYcGq3HWZFT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path of our video\n",
        "dir_video = '/content/Capstone-Updated/Videos/camera30fps.mp4'\n",
        "\n",
        "\n",
        "#put video in a variable\n",
        "video_cam_car = cv.VideoCapture(str(dir_video))\n",
        "ret, first_frame = video_cam_car.read()   #first frame is numpy array\n",
        "\n",
        "#number of frames of the video\n",
        "num_frames = 1000\n",
        "nb_row,nb_col,rgb = np.shape(first_frame)\n",
        "\n",
        "\n",
        "#empty these folders just in case they already exist and are not empty\n",
        "dir = '/content/Depth_pred_folder/inputs/'\n",
        "for f in os.listdir(dir):     #delete all files in the directory dir\n",
        "    os.remove(os.path.join(dir, f))\n",
        "\n",
        "dir = '/content/Depth_pred_folder/outputs/'\n",
        "for f in os.listdir(dir):     #delete all files in the directory dir\n",
        "    os.remove(os.path.join(dir, f))\n",
        "\n",
        "for k in range(0,num_frames):\n",
        "  _, frame_k = video_cam_car.read()   #read video\n",
        "\n",
        "##########################  run on the frame_k\n",
        "rgb_map_all_frames = frame_k   #fill the rgb map\n",
        "\n",
        "\n",
        "#saving the frame k as an image png\n",
        "frame_k_file = Image.fromarray(frame_k) #frame k is a np.array\n",
        "frame_k_file.save('/content/Depth_pred_folder/inputs/frame_k.png')\n",
        "\n",
        "# uncomment to run the segmentation code and fill the seg map\n",
        "#seg_map_all_frames[i] = MODEL.run(frame_k_file)\n",
        "\n",
        "### run boosted midas ###\n",
        "%cd /content/BoostingMonocularDepth/\n",
        "!python run.py --Final --data_dir /content/Depth_pred_folder/inputs/ --output_dir /content/Depth_pred_folder/outputs --depthNet 2\n",
        "\n",
        "%cd /content/\n",
        "#depth_im = Image.open('/content/Depth_pred_folder/outputs/frame_k.png')\n",
        "\n",
        "#depth_map_frame_k = np.array(depth_im)"
      ],
      "metadata": {
        "id": "2SW10Uv1ZIDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff28892d-f8da-4dd2-995d-0c1deca81efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BoostingMonocularDepth\n",
            "python3: can't open file 'run.py': [Errno 2] No such file or directory\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def boosted_midas(frame_k):   #run boosted midas on the frame_k and return a np.array depth map\n",
        "    #saving the frame k as an image png\n",
        "    frame_k_file = Image.fromarray(frame_k) #frame k is a np.array\n",
        "    frame_k_file.save('/content/Depth_pred_folder/inputs/frame_k.png')\n",
        "\n",
        "    ### run boosted midas ###\n",
        "    subp.call('cd /content/BoostingMonocularDepth/', shell=True)\n",
        "    call_args = 'python run.py --Final --data_dir /content/Depth_pred_folder/inputs/ --output_dir /content/Depth_pred_folder/outputs/ --depthNet 2'\n",
        "    call_args = call_args.split() # because call takes a list of strings\n",
        "    subp.call(call_args)\n",
        "\n",
        "    depth_im = Image.open('/content/Depth_pred_folder/outputs/frame_k.png')\n",
        "\n",
        "    depth_im = np.array(depth_im)\n",
        "\n",
        "    print(np.shape(depth_im))\n",
        "\n",
        "\n",
        "    return np.array(depth_im)"
      ],
      "metadata": {
        "id": "j2xPLso7ciKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja3q7PyhB4H"
      },
      "source": [
        "# Conversion to meters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Data\n",
        "stored it in a pickle file"
      ],
      "metadata": {
        "id": "KHrFRl9Ag2IK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoPzjnl5hCCS"
      },
      "outputs": [],
      "source": [
        "# DATA from the experiments\n",
        "\n",
        "equiv=[[0,200],    #for extrapolation\n",
        "      [0.001,51],    #for extrapolation\n",
        "      [0.1,45],     #premier plan\n",
        "      [0.9,42.3],\n",
        "      [1.8,37.4],\n",
        "      [2.7,28.7],\n",
        "      [3.6,24.443365],\n",
        "      [4.5,22.058018],\n",
        "      [5.4,15.317413],\n",
        "      [6.3,14.677493],\n",
        "      [7.2,10.969739],\n",
        "      [8.1,10.883035],\n",
        "      [9,9.883035],\n",
        "      [9.9,8.058806],\n",
        "      [10.8,7.5158963],\n",
        "      [11.7,7.098169],\n",
        "      [12.6,6.111024],\n",
        "      [13.5,5.6323136],\n",
        "      [14.4,5.2216917],\n",
        "      [15.3,5],\n",
        "      [16.2,4.9529667],\n",
        "      [17.1,4.8],\n",
        "      [18,4.7],\n",
        "      [18.9,4.6],\n",
        "      [19.8,4.5],\n",
        "      [20.7,4.4],\n",
        "      [21.6,4.3],\n",
        "      [22.5,4.2],\n",
        "      [23.4,4.1],\n",
        "      [24.3,4],\n",
        "      [25.2,3.9],\n",
        "      [26.1,3.8],\n",
        "      [27,3.7],\n",
        "      [27.9,3.6],\n",
        "      [28.8,3.5],\n",
        "      [29.7,3.2],\n",
        "      [30.6,3],\n",
        "      [40,1.98],\n",
        "      [50,1.33],\n",
        "      [60,0.0], #horizon\n",
        "\n",
        "      [120,-20]    #horizon extended\n",
        "\n",
        "       ]   #for extrapolation\n",
        "\n",
        "#=========================================================================================\n",
        "\n",
        "equiv2=[[1,41.05157], #1yard  0302\n",
        "        [1,42.18351],\n",
        "        [1,31.304607],\n",
        "        [1,25.090006],\n",
        "        [1,23.275448], #5yard 0306\n",
        "        [1,19.171278],\n",
        "        [1,17.472866],\n",
        "        [1,16.775742],\n",
        "        [1,15.820402],\n",
        "        [1,15.538459], #10yard  0311\n",
        "        [1,14.466544],\n",
        "        [1,12.707126],\n",
        "        [1,10.957558],\n",
        "\n",
        "\n",
        "        [1,6.023936],#'''inacurrate'''\n",
        "        [1,9.797453],  #15yard 0316\n",
        "        [1,7.2150397],\n",
        "        [1,6.3944836],\n",
        "        [1,8.514687],\n",
        "        [1,7.735209],\n",
        "        ]\n",
        "for i in range(len(equiv2)):\n",
        "  equiv2[i][0]*=(i+1)*0.9144   #1 yard = 0.9144 m\n",
        "\n",
        "equiv=equiv+equiv2\n",
        "\n",
        "# take second element for sort\n",
        "def takeSecond(elem):\n",
        "    return elem[1]\n",
        "# sort list with key\n",
        "equiv.sort(key=takeSecond)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create pickle file to store the measurements\n",
        "file_name=\"data_midas_2_meter_0.pkl\"\n",
        "df_data_exp = pd.DataFrame(data=equiv, columns=['Meters', 'Midas Values'])\n",
        "df_data_exp.to_pickle(file_name)"
      ],
      "metadata": {
        "id": "qEP2U7x2hJhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conv function\n",
        "use polynomial of 10deg regression"
      ],
      "metadata": {
        "id": "vxQZQBjhhjP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read pickle file of the experimental measurements\n",
        "file_name=\"data_midas_2_meter_0.pkl\"\n",
        "df2 = pd.read_pickle(file_name)\n",
        "X_conv_2_mt = df2['Midas Values'].to_numpy()\n",
        "Y_conv_2_mt = df2['Meters'].to_numpy()\n",
        "\n",
        "\n",
        "conv = np.poly1d(np.polyfit(X_conv_2_mt, Y_conv_2_mt, 10))\n",
        "\n",
        "\n",
        "plt.plot(X_conv_2_mt, Y_conv_2_mt, '+', X_conv_2_mt, conv(X_conv_2_mt), '-')\n",
        "plt.xlabel('Midas Values')\n",
        "plt.ylabel('Distance in Meters')\n",
        "plt.legend(['data', 'Regression'], loc = 'best')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "zycoF0fXhLTU",
        "outputId": "27a70fce-3d08-4629-841a-498e57b69df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error of exp data\n"
      ],
      "metadata": {
        "id": "19vfV2bahPPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code to determine the mean of the error and the variance of the error\n",
        "def Mean_Var_exp(X,Y):\n",
        "  Var_err_exp = 0\n",
        "  Mean_err_exp = 0\n",
        "  nb_points = len(X)\n",
        "  for i in range(nb_points):\n",
        "    Mean_err_exp += (Y[i]-conv(X[i]))/ nb_points\n",
        "  for i in range(nb_points):\n",
        "    Var_err_exp += (Y[i]-conv(X[i])-Mean_err_exp)**2/ nb_points\n",
        "  return Mean_err_exp, Var_err_exp\n",
        "Mean_Var_exp(X_conv_2_mt,Y_conv_2_mt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O13J3KNthLVZ",
        "outputId": "09542640-51ea-47c3-c098-3c27acb3febf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.861916456595152e-08, 4.007766522662304)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmSaT684v_VA"
      },
      "source": [
        "###Increased contrast\n",
        "Just to get increased contrast, not values in meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "5dbd5aff-37fb-421e-9d2a-5529bf5af3de",
        "id": "Gbwkqe0Gv_VA"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-d295d92c01e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoef_expand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#midas output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_expand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;31m#meters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0;32m--> 314\u001b[0;31m                                  \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'interpolate'"
          ]
        }
      ],
      "source": [
        "# coef_expand=[[-5,-1],\n",
        "#         [1,10],\n",
        "#         [5,25],\n",
        "#         [10,35],\n",
        "#         [15,42],\n",
        "#         [35,43],\n",
        "#         [45,50],\n",
        "#         [51,51]\n",
        "#         ]\n",
        "# coef_expand=np.asarray(coef_expand)\n",
        "# X = coef_expand[:,0]    #midas output\n",
        "# Y=coef_expand[:,1]      #meters\n",
        "# expand=np.interpolate.interp1d(X, Y, kind='cubic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDq3tgxhQV4"
      },
      "outputs": [],
      "source": [
        "coef_expand=[[-5,-1],\n",
        "        [1,10],\n",
        "        [5,25],\n",
        "        [10,35],\n",
        "        [15,42],\n",
        "        [35,43],\n",
        "        [45,50],\n",
        "        [51,51]\n",
        "        ]\n",
        "coef_expand=np.asarray(coef_expand)\n",
        "X = coef_expand[:,0]    #midas output\n",
        "Y=coef_expand[:,1]      #meters\n",
        "expand=sp.interpolate.interp1d(X, Y, kind='cubic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99un1SR_xll"
      },
      "source": [
        "# Segmented Point Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-QtEEGNGcuH"
      },
      "outputs": [],
      "source": [
        "from math import sin,cos,atan2\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "#return the map of distances [X,Y,Z] for each pixel for certain object using the midas + segmentation maps\n",
        "def depth2pcd_segm(depth,seg_map):\n",
        "    # print(depth)\n",
        "    # print(seg_map)\n",
        "    width = depth.shape[1]\n",
        "    height = depth.shape[0]\n",
        "    # print(width)\n",
        "    # print(height)\n",
        "    fx= 926.9796142578125\n",
        "    fy= 924.431884765625\n",
        "    cx= 790.234375\n",
        "    cy= 617.5499267578125\n",
        "    points = []\n",
        "    objects_needed = {'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle', 'void'}\n",
        "    objects_car = {'car','sidewalk'}\n",
        "    objects_wanted = {'car', 'vegetation','sidewalk','fence'}\n",
        "\n",
        "\n",
        "    for v in range(0, width, 5):\n",
        "        for u in range(0, 1000, 5):\n",
        "            R = depth[u][v]\n",
        "            color = seg_map[u][v]\n",
        "\n",
        "            if R == 0:\n",
        "                continue\n",
        "\n",
        "            X_cam = (v - cx)\n",
        "            Y_cam = -(u - cy)\n",
        "\n",
        "            theta_x = atan2(X_cam,fx)\n",
        "            theta_y = atan2(Y_cam,fy)\n",
        "\n",
        "            X = R*cos(theta_y)*sin(theta_x)\n",
        "            Y = R*cos(theta_x)*sin(theta_y)\n",
        "            Z = R*cos(theta_x)*cos(theta_y)\n",
        "\n",
        "            if LABEL_NAMES[color] in objects_car:\n",
        "              points.append([X, Y, Z,color])\n",
        "\n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "refmmrWb_2eH"
      },
      "outputs": [],
      "source": [
        "def prediction_stream(image, seg_map, seg_data, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    for i in range(len(seg_map)):\n",
        "        for j in range(len(seg_map[i])):\n",
        "                seg_data[i][j] = LABEL_NAMES[seg_map[i][j]]\n",
        "\n",
        "\n",
        "\n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    output = (output > 0) * output\n",
        "    distanceGuess = 2\n",
        "    alpha = output[output.shape[0]-10, int(output.shape[1]*3/4)]*distanceGuess\n",
        "    output = alpha/(output+.001)\n",
        "    pc_3d = depth2pcd_segm(output,seg_map)\n",
        "    return pc_3d\n",
        "def prediction_video(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    seg_map = MODEL.run(original_im)\n",
        "    seg_data = np.full((len(seg_map),len(seg_map[0])),'nullvoidnada')\n",
        "    filled_seg_data = prediction_stream(original_im, seg_map, seg_data, frame, index)\n",
        "    # print(filled_seg_data)\n",
        "    return filled_seg_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-Kk28ldELA",
        "outputId": "1802e597-e0da-4729-a20e-06db6f95c50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Capstone-Updated/SAR+Camera_Fusion\n",
            "/content/Capstone-Updated/Videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  del sys.path[0]\n"
          ]
        }
      ],
      "source": [
        "# Get Map of Time frames of SAR to Camera, by finding nearest frames\n",
        "\n",
        "import pickle\n",
        "%cd /content/Capstone-Updated/SAR+Camera_Fusion\n",
        "\n",
        "with open('camera_times.pickle', 'rb') as file:\n",
        "    camera_times = pickle.load(file)\n",
        "%cd /content/Capstone-Updated/Videos\n",
        "\n",
        "with open('sar_tracklog.pickle', 'rb') as file:\n",
        "    SAR_tracklog = pickle.load(file)\n",
        "\n",
        "SAR_tracklog = np.array(SAR_tracklog)\n",
        "SAR_times = SAR_tracklog[:,0]\n",
        "\n",
        "timestamp_map = np.zeros(len(SAR_times))\n",
        "\n",
        "j=0\n",
        "for i in range(len(SAR_times)):\n",
        "  while camera_times[j]<SAR_times[i]:\n",
        "    j+=1\n",
        "  timestamp_map[i] = j\n",
        "# print(timestamp_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5OVRWSXYUo4"
      },
      "source": [
        "# Convert Camera Frame to SAR frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe_Qnn-DYafi"
      },
      "outputs": [],
      "source": [
        "#Transform a point [X,Y,Z] from the camera frame to the car frame (SAR)\n",
        "def Cam_ref_2_Car_ref(Pos_obj_cam):\n",
        "    #camera extrinsic (quaternion, translation)\n",
        "    R=[[ 0.99994752,  0.00325207,  0.00971481],\n",
        "    [-0.0030831 ,  0.99984459, -0.01735761],\n",
        "    [-0.00976975,  0.01732675,  0.99980215]]\n",
        "\n",
        "    T=[-0.41649988293647766, 0.09146018326282501, 0.011436160653829575]\n",
        "\n",
        "    Pos_obj_car = R@Pos_obj_cam[:3] + T\n",
        "    Pos_obj_car = np.append(Pos_obj_car,[Pos_obj_cam[-1]])\n",
        "    return Pos_obj_car"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKc6MK8Hy_YJ"
      },
      "source": [
        "# SAR Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ32WSdJzDmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb683b0-f5be-4f8a-c09d-f5f884f797ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Capstone-Updated/Videos\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Capstone-Updated/Videos\n",
        "\n",
        "RADAR_VIDEO = 'radar_sar.mp4'\n",
        "CAMERA_VIDEO = 'camera.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqhiiwnSHaB8"
      },
      "outputs": [],
      "source": [
        "def Convert_to_Meters(frame):\n",
        "  center_x = 625\n",
        "  center_y = 624\n",
        "  height, width = frame.shape\n",
        "\n",
        "  points = []\n",
        "\n",
        "  for v in range(0, width):\n",
        "        for u in range(0, height):\n",
        "          if frame[u][v]<200:\n",
        "            x = (v-center_x)*0.04\n",
        "            y = -(u-center_y)*0.04\n",
        "            points.append([x,y])\n",
        "  return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlWuU-x-zXSF"
      },
      "outputs": [],
      "source": [
        "#Threshold the SAR pointcloud\n",
        "def GetThreshold_Binary(frame):\n",
        "  gray_scale = 255 - cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
        "  threshold = 0.9*np.max(gray_scale)\n",
        "  _, thres = cv.threshold(gray_scale, threshold, 255,cv.THRESH_BINARY)\n",
        "  point_cloud = Convert_to_Meters(thres)\n",
        "  point_cloud = np.array(point_cloud)\n",
        "  return point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o--rPYmC54_a"
      },
      "outputs": [],
      "source": [
        "def Plot_Camera_with_SAR(point_cloud_SAR, point_cloud_Camera):\n",
        "\n",
        "  fig = plt.figure(figsize=(12,6))\n",
        "  plt.scatter(point_cloud_SAR[:,0],point_cloud_SAR[:,1],s=0.1, color = 'black')\n",
        "\n",
        "  if len(point_cloud_Camera)>0:\n",
        "    color = (point_cloud_Camera[:, 3])\n",
        "    color_plot = np.array([COLOR_MAP[int(c)] for c in color])\n",
        "    # print(color_plot)\n",
        "    plt.scatter(point_cloud_Camera[:,0],point_cloud_Camera[:,2],s=0.1 , color = color_plot/255)\n",
        "  plt.xlabel('Z')\n",
        "  plt.ylabel('X')\n",
        "  plt.xlim(-25, 25)\n",
        "  plt.ylim(0, 25)\n",
        "\n",
        "  plt.savefig('saved_figure.jpg')\n",
        "  im = cv.imread('saved_figure.jpg')\n",
        "  frames.append(im)\n",
        "  plt.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Code"
      ],
      "metadata": {
        "id": "3B04gmGDrfIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "frames=[]\n",
        "video_radar = cv.VideoCapture(RADAR_VIDEO)\n",
        "video_camera = cv.VideoCapture(CAMERA_VIDEO)\n",
        "nb_frames_download_video = 100\n",
        "\n",
        "\n",
        "_, first_frame = video_radar.read()\n",
        "height, width, layers = first_frame.shape\n",
        "size = (width, height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "download_video = cv.VideoWriter('SARandCamera_pointcloud.avi', fourcc, 30.0, size)\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "    for i in range(nb_frames_download_video):\n",
        "        print(i)\n",
        "\n",
        "        # Get SAR frame\n",
        "        _, frame_radar = video_radar.read()\n",
        "        if not _: break\n",
        "        points_SAR= GetThreshold_Binary(frame_radar)\n",
        "\n",
        "        # Get camera frame\n",
        "        video_camera.set(cv.CAP_PROP_POS_FRAMES, int(timestamp_map[i]))\n",
        "        _, frame_camera = video_camera.read()\n",
        "        pc3darray = prediction_video(frame_camera, i)\n",
        "\n",
        "        # Change to SAR frame\n",
        "        pointcloud = []\n",
        "        for j in range(len(pc3darray)):\n",
        "          pointcloud.append(Cam_ref_2_Car_ref(pc3darray[j]))\n",
        "        points_camera = np.array(pointcloud)\n",
        "\n",
        "        # Plot together\n",
        "        Plot_Camera_with_SAR(points_SAR, points_camera)\n",
        "\n",
        "        IPython.display.clear_output(wait=True)\n",
        "\n",
        "        #add frame to the video of the top down view\n",
        "        download_video.write(frames[i])\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "download_video.release()\n",
        "\n",
        "\n",
        "# height, width, layers = frames[0].shape\n",
        "# size = (width, height)\n",
        "# fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "# download_video = cv.VideoWriter('SARandCamera_pointcloud.avi', fourcc, 30.0, size)\n",
        "\n",
        "# for i in range(len(frames)):\n",
        "#     out.write(frames[i])\n",
        "# out.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqtwr-zVrdZh",
        "outputId": "597294d2-c052-4114-d74c-219a976f6644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bjBDj97y9k18",
        "e_9rEI7rvhwW",
        "wVnUbIVY96HU",
        "uCBzhadMUZGX",
        "Kja3q7PyhB4H",
        "q99un1SR_xll",
        "V5OVRWSXYUo4"
      ],
      "name": "SAR_and_Camera_PointCloud_CombinedCode_26April.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "197cda64e0e04cdc85e8befa58d9370f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd830649fc8042cdb7b3adf0162cd8bc",
              "IPY_MODEL_f82bc16105b94f389c2fc136813415fb",
              "IPY_MODEL_64b06064c97c4a9ab9b50d932fb55ae7"
            ],
            "layout": "IPY_MODEL_3fe4a1d3b676440a9a2bb13168334f5a"
          }
        },
        "bd830649fc8042cdb7b3adf0162cd8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ba942f3aa04b658652a452e4b7fa27",
            "placeholder": "​",
            "style": "IPY_MODEL_606e0c079a624727bc5a835edaf5a09d",
            "value": "100%"
          }
        },
        "f82bc16105b94f389c2fc136813415fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17fa647c640c40ab885909ed7bb3b6ce",
            "max": 1376378527,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12ffc61f5f43434d8bf16f06e1a09d21",
            "value": 1376378527
          }
        },
        "64b06064c97c4a9ab9b50d932fb55ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cd3d9150c35424a976ff9ce062f054f",
            "placeholder": "​",
            "style": "IPY_MODEL_1a2adeafa2a24bb090e163539214b04f",
            "value": " 1.28G/1.28G [01:02&lt;00:00, 10.6MB/s]"
          }
        },
        "3fe4a1d3b676440a9a2bb13168334f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ba942f3aa04b658652a452e4b7fa27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "606e0c079a624727bc5a835edaf5a09d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17fa647c640c40ab885909ed7bb3b6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ffc61f5f43434d8bf16f06e1a09d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cd3d9150c35424a976ff9ce062f054f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2adeafa2a24bb090e163539214b04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}